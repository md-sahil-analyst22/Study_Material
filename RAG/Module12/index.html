<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Complete MM-RAG & Multimodal AI Study Guide</title>
    <style>
        :root {
            --primary-color: #6366f1;
            --secondary-color: #ec4899;
            --tertiary-color: #14b8a6;
            --bg-color: #f8fafc;
            --card-bg: #ffffff;
            --text-primary: #1e293b;
            --text-secondary: #64748b;
            --border-color: #e2e8f0;
            --code-bg: #1e293b;
            --code-text: #e2e8f0;
            --success-color: #10b981;
            --warning-color: #f59e0b;
            --error-color: #ef4444;
            --spacing-xs: 0.25rem;
            --spacing-sm: 0.5rem;
            --spacing-md: 1rem;
            --spacing-lg: 1.5rem;
            --spacing-xl: 2rem;
            --spacing-2xl: 3rem;
            --radius-sm: 0.375rem;
            --radius-md: 0.5rem;
            --radius-lg: 0.75rem;
            --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
            --shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
            --shadow-lg: 0 10px 15px -3px rgba(0, 0, 0, 0.1);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background-color: var(--bg-color);
            color: var(--text-primary);
            line-height: 1.6;
        }

        /* Navigation Styles */
        nav {
            position: sticky;
            top: 0;
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color) 100%);
            color: white;
            padding: var(--spacing-md) var(--spacing-lg);
            box-shadow: var(--shadow-lg);
            z-index: 100;
        }

        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: var(--spacing-md);
        }

        .nav-title {
            font-size: 1.5rem;
            font-weight: 700;
            letter-spacing: -0.5px;
        }

        .nav-toggle {
            display: none;
            background: none;
            border: none;
            color: white;
            font-size: 1.5rem;
            cursor: pointer;
        }

        .nav-menu {
            display: flex;
            gap: var(--spacing-lg);
            list-style: none;
            flex-wrap: wrap;
        }

        .nav-menu a {
            color: white;
            text-decoration: none;
            font-size: 0.95rem;
            padding: var(--spacing-sm) var(--spacing-md);
            border-radius: var(--radius-sm);
            transition: all 0.3s ease;
        }

        .nav-menu a:hover {
            background-color: rgba(255, 255, 255, 0.2);
            transform: translateY(-2px);
        }

        /* Header Section */
        .header {
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--tertiary-color) 100%);
            color: white;
            padding: var(--spacing-2xl) var(--spacing-lg);
            text-align: center;
            box-shadow: var(--shadow-lg);
        }

        .header h1 {
            font-size: clamp(2rem, 5vw, 3rem);
            margin-bottom: var(--spacing-md);
            font-weight: 800;
            letter-spacing: -1px;
            color: white;
        }

        .header p {
            font-size: 1.1rem;
            opacity: 0.95;
            max-width: 800px;
            margin: 0 auto;
            color: white;
        }

        /* Container */
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: var(--spacing-lg);
        }

        /* Section Styles */
        section {
            margin-bottom: var(--spacing-2xl);
        }

        h2 {
            font-size: clamp(1.5rem, 4vw, 2.2rem);
            color: var(--primary-color);
            margin-bottom: var(--spacing-lg);
            padding-bottom: var(--spacing-md);
            border-bottom: 3px solid var(--secondary-color);
            position: relative;
        }

        h2::before {
            content: '';
            position: absolute;
            bottom: -3px;
            left: 0;
            width: 60px;
            height: 3px;
            background: var(--secondary-color);
        }

        h3 {
            font-size: 1.3rem;
            color: var(--secondary-color);
            margin: var(--spacing-lg) 0 var(--spacing-md) 0;
            display: flex;
            align-items: center;
            gap: var(--spacing-md);
        }

        h4 {
            font-size: 1.1rem;
            color: var(--tertiary-color);
            margin: var(--spacing-lg) 0 var(--spacing-md) 0;
            font-weight: 600;
        }

        /* Card Styles */
        .card {
            background: var(--card-bg);
            border-radius: var(--radius-lg);
            padding: var(--spacing-lg);
            margin-bottom: var(--spacing-lg);
            box-shadow: var(--shadow-md);
            border-left: 4px solid var(--primary-color);
            transition: all 0.3s ease;
        }

        .card:hover {
            transform: translateY(-4px);
            box-shadow: var(--shadow-lg);
        }

        .card.warning {
            border-left-color: var(--warning-color);
            background-color: rgba(245, 158, 11, 0.05);
        }

        .card.success {
            border-left-color: var(--success-color);
            background-color: rgba(16, 185, 129, 0.05);
        }

        .card.error {
            border-left-color: var(--error-color);
            background-color: rgba(239, 68, 68, 0.05);
        }

        .card.info {
            border-left-color: var(--tertiary-color);
            background-color: rgba(20, 184, 166, 0.05);
        }

        /* Grid Layout */
        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: var(--spacing-lg);
            margin-bottom: var(--spacing-lg);
        }

        /* Diagram Container */
        .diagram-container {
            background: var(--card-bg);
            border-radius: var(--radius-lg);
            padding: var(--spacing-lg);
            margin: var(--spacing-lg) 0;
            border: 2px solid var(--border-color);
            overflow-x: auto;
            box-shadow: var(--shadow-md);
            text-align: center;
        }

        .diagram-title {
            font-weight: 600;
            color: var(--primary-color);
            margin-bottom: var(--spacing-md);
            font-size: 1.1rem;
        }

        .diagram-container img {
            max-width: 100%;
            height: auto;
            border-radius: var(--radius-md);
            margin-top: var(--spacing-md);
        }

        /* Code Block Styles */
        .code-block {
            background-color: var(--code-bg);
            color: var(--code-text);
            padding: var(--spacing-lg);
            border-radius: var(--radius-lg);
            overflow-x: auto;
            margin: var(--spacing-lg) 0;
            border: 1px solid #404854;
            position: relative;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.5;
            box-shadow: var(--shadow-md);
        }

        .code-block.python {
            border-left: 4px solid #3776ab;
        }

        .code-block.html {
            border-left: 4px solid #f16529;
        }

        .code-block.javascript {
            border-left: 4px solid #f7df1e;
            color: #333;
            background-color: #f5f5f5;
        }

        .code-label {
            position: absolute;
            top: var(--spacing-sm);
            right: var(--spacing-lg);
            background-color: rgba(255, 255, 255, 0.15);
            color: var(--code-text);
            padding: var(--spacing-xs) var(--spacing-md);
            border-radius: var(--radius-sm);
            font-size: 0.75rem;
            font-weight: 600;
            text-transform: uppercase;
        }

        pre {
            margin: 0;
            white-space: pre-wrap;
            word-wrap: break-word;
            padding-top: var(--spacing-lg);
        }

        code {
            font-family: 'Courier New', monospace;
        }

        /* Inline Code */
        p code, li code, td code {
            background-color: rgba(99, 102, 241, 0.1);
            color: var(--primary-color);
            padding: var(--spacing-xs) var(--spacing-sm);
            border-radius: var(--radius-sm);
            font-size: 0.9em;
        }

        /* List Styles */
        ul, ol {
            margin: var(--spacing-lg) 0 var(--spacing-lg) var(--spacing-xl);
        }

        li {
            margin-bottom: var(--spacing-md);
            color: var(--text-primary);
        }

        li p {
            margin: var(--spacing-sm) 0;
        }

        /* Table Styles */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: var(--spacing-lg) 0;
            box-shadow: var(--shadow-sm);
            border-radius: var(--radius-lg);
            overflow: hidden;
        }

        thead {
            background-color: var(--primary-color);
            color: white;
        }

        th, td {
            padding: var(--spacing-md) var(--spacing-lg);
            text-align: left;
            border-bottom: 1px solid var(--border-color);
        }

        tbody tr:nth-child(even) {
            background-color: rgba(99, 102, 241, 0.03);
        }

        tbody tr:hover {
            background-color: rgba(99, 102, 241, 0.08);
            transition: background-color 0.2s ease;
        }

        /* Feature Box */
        .feature-box {
            display: flex;
            gap: var(--spacing-lg);
            padding: var(--spacing-lg);
            background: var(--card-bg);
            border-radius: var(--radius-lg);
            margin-bottom: var(--spacing-lg);
            box-shadow: var(--shadow-sm);
        }

        .feature-icon {
            font-size: 2.5rem;
            min-width: 60px;
            text-align: center;
        }

        .feature-content h4 {
            margin-top: 0;
        }

        /* Comparison Table */
        .comparison-table {
            width: 100%;
            margin: var(--spacing-lg) 0;
        }

        .comparison-table td {
            padding: var(--spacing-md);
            vertical-align: top;
        }

        /* Badge Styles */
        .badge {
            display: inline-block;
            padding: var(--spacing-xs) var(--spacing-md);
            border-radius: var(--radius-lg);
            font-size: 0.85rem;
            font-weight: 600;
            margin-right: var(--spacing-sm);
            margin-bottom: var(--spacing-sm);
        }

        .badge-primary {
            background-color: rgba(99, 102, 241, 0.2);
            color: var(--primary-color);
        }

        .badge-secondary {
            background-color: rgba(236, 72, 153, 0.2);
            color: var(--secondary-color);
        }

        .badge-success {
            background-color: rgba(16, 185, 129, 0.2);
            color: var(--success-color);
        }

        .badge-warning {
            background-color: rgba(245, 158, 11, 0.2);
            color: var(--warning-color);
        }

        /* Highlight Box */
        .highlight-box {
            background: linear-gradient(135deg, rgba(99, 102, 241, 0.1) 0%, rgba(20, 184, 166, 0.1) 100%);
            border-left: 4px solid var(--primary-color);
            padding: var(--spacing-lg);
            border-radius: var(--radius-lg);
            margin: var(--spacing-lg) 0;
        }

        .highlight-box strong {
            color: var(--primary-color);
        }

        /* Footer */
        footer {
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color) 100%);
            color: white;
            text-align: center;
            padding: var(--spacing-2xl) var(--spacing-lg);
            margin-top: var(--spacing-2xl);
            box-shadow: var(--shadow-lg);
        }

        footer p {
            margin-bottom: var(--spacing-md);
            color: white;
        }

        footer a {
            color: white;
            text-decoration: underline;
        }

        footer a:hover {
            color: #e2e8f0;
        }

        /* TOC Styles */
        .toc {
            background: var(--card-bg);
            padding: var(--spacing-lg);
            border-radius: var(--radius-lg);
            margin-bottom: var(--spacing-2xl);
            box-shadow: var(--shadow-md);
            border-left: 4px solid var(--tertiary-color);
        }

        .toc h3 {
            margin-top: 0;
            color: var(--tertiary-color);
        }

        .toc ul {
            list-style-position: inside;
        }

        .toc a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.3s ease;
        }

        .toc a:hover {
            color: var(--secondary-color);
            text-decoration: underline;
        }

        /* Back to Top Button */
        .back-to-top {
            position: fixed;
            bottom: var(--spacing-lg);
            right: var(--spacing-lg);
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color) 100%);
            color: white;
            border: none;
            border-radius: 50%;
            width: 50px;
            height: 50px;
            font-size: 1.5rem;
            cursor: pointer;
            display: none;
            align-items: center;
            justify-content: center;
            box-shadow: var(--shadow-lg);
            transition: all 0.3s ease;
            z-index: 99;
        }

        .back-to-top:hover {
            transform: translateY(-4px);
            box-shadow: var(--shadow-lg);
        }

        .back-to-top.show {
            display: flex;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            .nav-menu {
                flex-direction: column;
                gap: var(--spacing-sm);
            }

            .nav-toggle {
                display: block;
            }

            h2 {
                font-size: 1.5rem;
            }

            h3 {
                font-size: 1.1rem;
            }

            .grid {
                grid-template-columns: 1fr;
            }

            .feature-box {
                flex-direction: column;
                text-align: center;
            }

            .feature-icon {
                min-width: auto;
            }

            table {
                font-size: 0.9rem;
            }

            th, td {
                padding: var(--spacing-md) var(--spacing-sm);
            }

            .code-block {
                padding: var(--spacing-md);
                font-size: 0.85rem;
            }

            .container {
                padding: var(--spacing-md);
            }

            .header {
                padding: var(--spacing-lg);
            }

            .header h1 {
                font-size: 1.8rem;
            }
        }

        @media (max-width: 480px) {
            .nav-container {
                flex-direction: column;
                gap: var(--spacing-md);
            }

            .nav-title {
                font-size: 1.2rem;
            }

            h2 {
                font-size: 1.3rem;
            }

            h3 {
                font-size: 1rem;
                flex-direction: column;
                gap: var(--spacing-sm);
            }

            .card {
                padding: var(--spacing-md);
            }

            .code-block {
                padding: var(--spacing-md);
                font-size: 0.8rem;
                margin: var(--spacing-md) 0;
            }

            .badge {
                display: block;
                margin-bottom: var(--spacing-sm);
            }
        }

        /* Animation Styles */
        @keyframes slideInUp {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .card {
            animation: slideInUp 0.6s ease-out;
        }

        /* SVG Styles for Diagrams */
        svg {
            max-width: 100%;
            height: auto;
        }

        /* Mermaid Diagram Styles */
        .mermaid {
            display: flex;
            justify-content: center;
            margin: var(--spacing-lg) 0;
        }

        /* Split Layout for Code and Content */
        .content-split {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: var(--spacing-lg);
            align-items: start;
        }

        @media (max-width: 1000px) {
            .content-split {
                grid-template-columns: 1fr;
            }
        }

        /* Implementation Steps */
        .steps-container {
            counter-reset: step-counter;
        }

        .step {
            counter-increment: step-counter;
            margin-bottom: var(--spacing-lg);
            padding-left: var(--spacing-xl);
            position: relative;
        }

        .step::before {
            content: counter(step-counter);
            position: absolute;
            left: 0;
            top: 0;
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color) 100%);
            color: white;
            width: 30px;
            height: 30px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 600;
            font-size: 0.9rem;
        }

        /* Pros and Cons List */
        .pros-cons {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: var(--spacing-lg);
        }

        .pros, .cons {
            padding: var(--spacing-lg);
            border-radius: var(--radius-lg);
            background: var(--card-bg);
        }

        .pros {
            border-left: 4px solid var(--success-color);
            background-color: rgba(16, 185, 129, 0.05);
        }

        .cons {
            border-left: 4px solid var(--error-color);
            background-color: rgba(239, 68, 68, 0.05);
        }

        .pros h4, .cons h4 {
            margin-top: 0;
        }

        @media (max-width: 768px) {
            .pros-cons {
                grid-template-columns: 1fr;
            }
        }

        /* Expandable Sections */
        .accordion {
            margin-bottom: var(--spacing-lg);
        }

        .accordion-item {
            background: var(--card-bg);
            border: 1px solid var(--border-color);
            border-radius: var(--radius-lg);
            margin-bottom: var(--spacing-md);
            overflow: hidden;
        }

        .accordion-header {
            padding: var(--spacing-lg);
            cursor: pointer;
            background: linear-gradient(135deg, rgba(99, 102, 241, 0.05) 0%, rgba(20, 184, 166, 0.05) 100%);
            border: none;
            width: 100%;
            text-align: left;
            font-weight: 600;
            color: var(--primary-color);
            transition: all 0.3s ease;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .accordion-header:hover {
            background: linear-gradient(135deg, rgba(99, 102, 241, 0.1) 0%, rgba(20, 184, 166, 0.1) 100%);
        }

        .accordion-header.active {
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--tertiary-color) 100%);
            color: white;
        }

        .accordion-content {
            padding: var(--spacing-lg);
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease;
            background: var(--card-bg);
        }

        .accordion-content.show {
            max-height: 2000px;
        }

        /* Keyword Highlight */
        .keyword {
            color: var(--primary-color);
            font-weight: 600;
        }

        .keyword-secondary {
            color: var(--secondary-color);
            font-weight: 600;
        }

        .keyword-tertiary {
            color: var(--tertiary-color);
            font-weight: 600;
        }
    </style>
</head>
<body>
 
    <!-- Header -->
    <div class="header">
        <h1>ü§ñ Multimodal Retrieval-Augmented Generation (MM-RAG)</h1>
        <p>A Comprehensive Study Guide on Building Intelligent AI Systems with Multiple Data Types</p>
    </div>

    <!-- Main Content -->
    <div class="container">
        <!-- Table of Contents -->
        <div class="toc">
            <h3>üìñ Quick Navigation</h3>
            <ul>
                <li><a href="#introduction">Introduction & Overview</a></li>
                <li><a href="#what-is">What is MM-RAG?</a></li>
                <li><a href="#why-mmrag">Why MM-RAG Matters</a></li>
                <li><a href="#core-concept">Core Concepts & Principles</a></li>
                <li><a href="#architecture">System Architecture</a></li>
                <li><a href="#pipeline">Four-Step Pipeline</a></li>
                <li><a href="#multimodal-chatbots">Multimodal Chatbots & QA Systems</a></li>
                <li><a href="#examples">Real-World Examples</a></li>
                <li><a href="#implementation">Implementation Guide</a></li>
                <li><a href="#usecases">Use Cases & Applications</a></li>
                <li><a href="#features">Key Features & USPs</a></li>
                <li><a href="#pros-cons">Pros & Cons</a></li>
                <li><a href="#alternatives">Alternatives & Comparisons</a></li>
                <li><a href="#real-life">Real-Life Implementations</a></li>
                <li><a href="#future">Future Trends & Scope</a></li>
            </ul>
        </div>

        <!-- INTRODUCTION SECTION -->
        <section id="introduction">
            <h2>üìå Introduction & Overview</h2>
            
            <div class="card">
                <h3>üéØ Welcome to the Future of AI</h3>
                <p>
                    Multimodal AI represents a paradigm shift in how machines understand and process information. 
                    Unlike traditional AI systems that work with a single type of data (text or images), multimodal 
                    AI systems can <span class="keyword">simultaneously process and integrate multiple data types</span> 
                    including text, images, audio, and video. This comprehensive capability brings AI closer to how humans 
                    naturally perceive and understand the world.
                </p>
            </div>

            <div class="grid">
                <div class="card info">
                    <h4>üìä The Problem We're Solving</h4>
                    <p>
                        Large Language Models (LLMs) are incredibly powerful but have a critical limitation: 
                        they don't naturally include domain-specific or proprietary data. When you ask an AI model 
                        about your company's fashion catalog, it can't answer because that data wasn't in its training set.
                    </p>
                </div>
                
                <div class="card success">
                    <h4>‚ú® The Solution: MM-RAG</h4>
                    <p>
                        MM-RAG solves this by combining Retrieval-Augmented Generation with multimodal processing. 
                        It retrieves relevant data from your databases and feeds it to the model, making responses 
                        accurate, contextual, and domain-aware.
                    </p>
                </div>

                <div class="card warning">
                    <h4>üöÄ Why It Matters Now</h4>
                    <p>
                        As AI becomes more integrated into business operations, the ability to work with real-world, 
                        multimodal data is no longer optional‚Äîit's essential. Fashion analysis needs images. Nutrition 
                        coaching needs food photos. Medical diagnosis needs imaging data.
                    </p>
                </div>
            </div>

            <div class="highlight-box">
                <strong>üîë Key Insight:</strong> MM-RAG bridges the gap between the power of large language models 
                and the specificity of proprietary data by augmenting LLM responses with retrieved, context-aware 
                information across multiple modalities.
            </div>
        </section>

        <!-- WHAT IS MM-RAG -->
        <section id="what-is">
            <h2>üéì What is MM-RAG (Multimodal Retrieval-Augmented Generation)?</h2>
            
            <div class="card">
                <h3>üìñ Definition</h3>
                <p>
                    <span class="keyword-secondary">Multimodal Retrieval-Augmented Generation (MM-RAG)</span> is an advanced 
                    AI technique that combines three powerful concepts:
                </p>
                <ul>
                    <li><strong>Multimodal:</strong> Systems working with multiple data types (text, images, audio, video)</li>
                    <li><strong>Retrieval-Augmented:</strong> Enhancing LLM responses by retrieving relevant information from databases</li>
                    <li><strong>Generation:</strong> Creating detailed, accurate responses combining all modalities</li>
                </ul>
            </div>

            <div class="grid">
                <div class="feature-box">
                    <div class="feature-icon">üñºÔ∏è</div>
                    <div class="feature-content">
                        <h4>Multimodal Processing</h4>
                        <p>
                            Systems work with multiple data types or "modalities." Think of it as giving AI multiple senses 
                            - vision (images), hearing (audio), and text understanding all working together.
                        </p>
                    </div>
                </div>

                <div class="feature-box">
                    <div class="feature-icon">üîç</div>
                    <div class="feature-content">
                        <h4>Retrieval-Augmented</h4>
                        <p>
                            Instead of relying only on what the model learned during training, MM-RAG retrieves relevant 
                            information from your specific databases and datasets.
                        </p>
                    </div>
                </div>

                <div class="feature-box">
                    <div class="feature-icon">üí¨</div>
                    <div class="feature-content">
                        <h4>Generation</h4>
                        <p>
                            Using the retrieved context and multimodal data, the system generates comprehensive, 
                            accurate, and contextually relevant responses.
                        </p>
                    </div>
                </div>
            </div>

            <h3>üîÑ Three Core Patterns</h3>
            <table>
                <thead>
                    <tr>
                        <th>Pattern</th>
                        <th>Description</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Multimodal Data Retrieval</strong></td>
                        <td>Extracting relevant information from multiple data types simultaneously</td>
                        <td>Finding similar fashion items based on image matching while retrieving price data</td>
                    </tr>
                    <tr>
                        <td><strong>Contrastive Learning</strong></td>
                        <td>Creating embeddings that capture relationships between different modalities</td>
                        <td>Learning that the image of a red dress and the text "red dress" refer to the same item</td>
                    </tr>
                    <tr>
                        <td><strong>Multimodal Context</strong></td>
                        <td>Using information from all modalities to inform generation</td>
                        <td>Combining visual features, product descriptions, and pricing in the final response</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- WHY MM-RAG -->
        <section id="why-mmrag">
            <h2>üí° Why MM-RAG Matters</h2>

            <div class="card">
                <h3>üéØ The Traditional Problem</h3>
                <p>
                    <strong>Vision Models (like GPT-4V, Claude 3):</strong> Can see and analyze images but have limitations:
                </p>
                <ul>
                    <li>‚ùå Can't access your proprietary data</li>
                    <li>‚ùå Limited to general knowledge from training data</li>
                    <li>‚ùå Can't retrieve specific product information</li>
                    <li>‚ùå Can't connect images to your business databases</li>
                </ul>
            </div>

            <div class="card success">
                <h3>‚ú® The MM-RAG Solution</h3>
                <p>
                    <strong>MM-RAG:</strong> Fills the gap between vision and domain-specific knowledge:
                </p>
                <ul>
                    <li>‚úÖ Accesses your proprietary databases</li>
                    <li>‚úÖ Provides accurate, domain-specific information</li>
                    <li>‚úÖ Retrieves exact product details, pricing, availability</li>
                    <li>‚úÖ Integrates multiple data sources seamlessly</li>
                    <li>‚úÖ Maintains context across modalities</li>
                </ul>
            </div>

            <h3>üìä When to Use MM-RAG vs Alternatives</h3>
            <table>
                <thead>
                    <tr>
                        <th>Scenario</th>
                        <th>Use Vision Model</th>
                        <th>Use MM-RAG</th>
                        <th>Best Approach</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>General image description</td>
                        <td>‚úÖ</td>
                        <td>-</td>
                        <td>Vision Model</td>
                    </tr>
                    <tr>
                        <td>Domain-specific product matching</td>
                        <td>‚ö†Ô∏è Limited</td>
                        <td>‚úÖ</td>
                        <td>MM-RAG</td>
                    </tr>
                    <tr>
                        <td>Text-only question answering</td>
                        <td>-</td>
                        <td>‚úÖ</td>
                        <td>RAG (text only)</td>
                    </tr>
                    <tr>
                        <td>Complex business decisions with multiple data types</td>
                        <td>‚ùå</td>
                        <td>‚úÖ</td>
                        <td>MM-RAG</td>
                    </tr>
                    <tr>
                        <td>Real-time product catalog queries with images</td>
                        <td>‚ö†Ô∏è No DB access</td>
                        <td>‚úÖ</td>
                        <td>MM-RAG</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- CORE CONCEPTS -->
        <section id="core-concept">
            <h2>üß† Core Concepts & Principles</h2>

            <h3>1Ô∏è‚É£ Vector Embeddings: The Bridge Between Modalities</h3>
            <div class="content-split">
                <div>
                    <div class="card">
                        <h4>What Are Embeddings?</h4>
                        <p>
                            Embeddings are numerical representations of data that capture semantic meaning. 
                            An image of a red dress, the text "red dress", and the audio of someone saying 
                            "red dress" all get converted into vectors that are <strong>near each other in 
                            vector space</strong> if they mean the same thing.
                        </p>
                        <p>
                            Think of it like GPS coordinates where similar things are located nearby on a map, 
                            but instead of geographic space, it's semantic space.
                        </p>
                    </div>
                </div>
                <div>
                    <div class="highlight-box">
                        <strong>Vector Similarity Example:</strong>
                        <ul>
                            <li>Red dress image ‚Üí Vector [0.8, 0.2, 0.9, ...]</li>
                            <li>"Red dress" text ‚Üí Vector [0.78, 0.21, 0.88, ...]</li>
                            <li>Similarity score: 0.95 (Very similar!)</li>
                            <li><br/>Black jacket image ‚Üí Vector [0.1, 0.9, 0.3, ...]</li>
                            <li>Similarity score: 0.15 (Not similar)</li>
                        </ul>
                    </div>
                </div>
            </div>

            <h3>2Ô∏è‚É£ Cosine Similarity: Measuring Semantic Closeness</h3>
            <div class="card info">
                <h4>How It Works</h4>
                <p>
                    Cosine similarity measures the angle between two vectors. The closer the angle is to 0¬∞, 
                    the more similar they are. It ranges from -1 (opposite) to 1 (identical).
                </p>
                <div class="code-block python">
                    <span class="code-label">Python</span>
                    <pre>from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Example vectors for different items
red_dress_vector = np.array([0.8, 0.2, 0.9, 0.3, 0.7])
similar_dress_vector = np.array([0.78, 0.21, 0.88, 0.29, 0.72])
black_jacket_vector = np.array([0.1, 0.9, 0.3, 0.8, 0.2])

# Calculate similarity
similarity_1 = cosine_similarity(
    red_dress_vector.reshape(1, -1),
    similar_dress_vector.reshape(1, -1)
)[0][0]  # Output: ~0.9998 (nearly identical)

similarity_2 = cosine_similarity(
    red_dress_vector.reshape(1, -1),
    black_jacket_vector.reshape(1, -1)
)[0][0]  # Output: ~0.42 (somewhat different)</pre>
                </div>
            </div>

            <h3>3Ô∏è‚É£ Contrastive Learning: Connecting Modalities</h3>
            <div class="card">
                <h4>The Concept</h4>
                <p>
                    Contrastive learning trains models to understand that different representations 
                    (image, text, audio) of the same concept should have similar embeddings.
                </p>
                <div class="highlight-box">
                    <strong>Real Example - Fashion Item:</strong>
                    <p>A model learns that:</p>
                    <ul>
                        <li>An image of a red sweater</li>
                        <li>The text "red sweater"</li>
                        <li>The description "warm woolen garment in crimson color"</li>
                    </ul>
                    <p>...are all related and should have embeddings close to each other in vector space.</p>
                </div>
            </div>

            <h3>4Ô∏è‚É£ Pre-trained Models: Leveraging Existing Knowledge</h3>
            <table>
                <thead>
                    <tr>
                        <th>Model/Framework</th>
                        <th>Purpose</th>
                        <th>Example Use</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>ResNet50</strong></td>
                        <td>Image feature extraction</td>
                        <td>Converting fashion images to vectors</td>
                    </tr>
                    <tr>
                        <td><strong>CLIP</strong></td>
                        <td>Image-text alignment</td>
                        <td>Matching images with descriptions</td>
                    </tr>
                    <tr>
                        <td><strong>Llama 3.2 Vision</strong></td>
                        <td>Multimodal understanding</td>
                        <td>Analyzing images and generating detailed responses</td>
                    </tr>
                    <tr>
                        <td><strong>Sentence Transformers</strong></td>
                        <td>Text embeddings</td>
                        <td>Converting product descriptions to vectors</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- ARCHITECTURE SECTION -->
        <section id="architecture">
            <h2>üèóÔ∏è System Architecture</h2>

            <div class="card">
                <h3>üîß High-Level Architecture</h3>
                <p>
                    An MM-RAG system consists of interconnected components working together seamlessly:
                </p>
            </div>

            <div class="diagram-container">
                <div class="diagram-title">MM-RAG System Architecture</div>
                <img src="https://raw.githubusercontent.com/md-sahil-analyst22/Study_Material/main/images/MM_RAG.png" alt="MM-RAG System Architecture Diagram">
            </div>

            <h3>üîå Key Components Explained</h3>

            <div class="card">
                <h4>1. Input Processing Layer</h4>
                <p>
                    Different modalities require different processing:
                </p>
                <ul>
                    <li><strong>Images:</strong> Processed through neural networks (ResNet50, Vision Transformers)</li>
                    <li><strong>Text:</strong> Tokenized and processed through language models</li>
                    <li><strong>Audio:</strong> Converted to spectrograms or embeddings</li>
                    <li><strong>Video:</strong> Frames extracted and processed as images + temporal modeling</li>
                </ul>
            </div>

            <div class="card">
                <h4>2. Embedding Generation</h4>
                <p>
                    All inputs are converted to embeddings (dense vectors) using pre-trained models:
                </p>
                <div class="code-block python">
                    <span class="code-label">Python - Image to Embedding</span>
                    <pre>import torch
import torchvision.transforms as transforms
from torchvision.models import resnet50
from PIL import Image

# Load pre-trained ResNet50 model
model = resnet50(pretrained=True)
model.eval()

# Preprocessing pipeline
preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    ),
])

# Load and process image
image = Image.open('fashion_item.jpg')
input_tensor = preprocess(image).unsqueeze(0)

# Generate embedding (2048-dimensional vector)
with torch.no_grad():
    embedding = model(input_tensor)
    # Output: tensor of shape (1, 2048)</pre>
                </div>
            </div>

            <div class="card">
                <h4>3. Vector Database</h4>
                <p>
                    All embeddings are stored in a vector database for fast retrieval:
                </p>
                <ul>
                    <li><strong>Pinecone:</strong> Cloud-based vector database</li>
                    <li><strong>Weaviate:</strong> Open-source vector search</li>
                    <li><strong>Milvus:</strong> Scalable vector database</li>
                    <li><strong>FAISS:</strong> Facebook's similarity search library</li>
                </ul>
                <p>
                    These databases use optimized algorithms (like HNSW or IVF) to find similar embeddings 
                    in sub-millisecond time, even with millions of vectors.
                </p>
            </div>

            <div class="card">
                <h4>4. Retrieval & Ranking</h4>
                <p>
                    When a query comes in:
                </p>
                <ol>
                    <li>Query is converted to embedding using the same encoder</li>
                    <li>Vector database finds k nearest neighbors (e.g., top 5 similar items)</li>
                    <li>Results are ranked by similarity score (cosine similarity)</li>
                    <li>Top results are passed to the LLM with original metadata</li>
                </ol>
            </div>

            <div class="card success">
                <h4>5. Generation with LLM</h4>
                <p>
                    The retrieved context (images + text metadata) is combined with the original query 
                    and sent to a multimodal LLM:
                </p>
                <div class="code-block python">
                    <span class="code-label">Python - LLM with Retrieved Context</span>
                    <pre>messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "Analyze this outfit and suggest similar items"
            },
            {
                "type": "text",
                "text": f"Context from database: {retrieved_items_metadata}"
            },
            {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{encoded_image}"
                }
            }
        ]
    }
]

response = model.chat(messages=messages)
print(response['choices'][0]['message']['content'])</pre>
                </div>
            </div>
        </section>

        <!-- PIPELINE SECTION -->
        <section id="pipeline">
            <h2>‚öôÔ∏è The Four-Step MM-RAG Pipeline</h2>

            <div class="diagram-container">
                <div class="diagram-title">MM-RAG Pipeline Diagram</div>
                <img src="https://raw.githubusercontent.com/md-sahil-analyst22/Study_Material/main/images/MM_RAG_p1.png" alt="MM-RAG Pipeline Diagram">
            </div>

            <h3>üìã Detailed Pipeline Breakdown</h3>

            <div class="steps-container">
                <div class="step card">
                    <h4>Step 1: Data Indexing & Embedding</h4>
                    <p>
                        <strong>Objective:</strong> Prepare all your data for retrieval by converting it to embeddings 
                        and storing it in a vector database.
                    </p>
                    <p><strong>Process:</strong></p>
                    <ol>
                        <li>
                            <strong>Data Collection:</strong> Gather all your data‚Äîimages, text descriptions, 
                            metadata, pricing information, etc.
                        </li>
                        <li>
                            <strong>Preprocessing:</strong> Clean and format data. Resize images, tokenize text, 
                            normalize metadata.
                        </li>
                        <li>
                            <strong>Embedding Generation:</strong> Convert each data piece to embeddings using 
                            pre-trained encoders.
                        </li>
                        <li>
                            <strong>Indexing:</strong> Store embeddings in vector database with metadata links.
                        </li>
                    </ol>
                    <div class="code-block python">
                        <span class="code-label">Python - Indexing Process</span>
                        <pre>import pickle
import pandas as pd
import torch
from torchvision.models import resnet50

# Load dataset
df = pd.read_csv('fashion_items.csv')

# Initialize model
model = resnet50(pretrained=True)
model.eval()

# Generate embeddings for all items
embeddings = []
for idx, row in df.iterrows():
    image_path = row['image_path']
    image = Image.open(image_path)
    
    # Convert to embedding
    embedding = model(preprocess(image).unsqueeze(0))
    embeddings.append(embedding.cpu().numpy().flatten())
    
    if (idx + 1) % 100 == 0:
        print(f"Indexed {idx + 1} items")

# Save indexed data
df['embedding'] = embeddings
df.to_pickle('indexed_data.pkl')</pre>
                    </div>
                </div>

                <div class="step card">
                    <h4>Step 2: Data Retrieval</h4>
                    <p>
                        <strong>Objective:</strong> When a user submits a query, find the most relevant data 
                        from your database.
                    </p>
                    <p><strong>Process:</strong></p>
                    <ol>
                        <li>
                            <strong>Query Encoding:</strong> Convert the user's query (image, text, or multimodal) 
                            to embedding using the same encoder.
                        </li>
                        <li>
                            <strong>Similarity Search:</strong> Find k nearest neighbors (typically k=3-5) in vector space.
                        </li>
                        <li>
                            <strong>Ranking:</strong> Rank results by similarity score.
                        </li>
                        <li>
                            <strong>Filtering:</strong> Apply any domain-specific filters (e.g., price range, 
                            availability).
                        </li>
                    </ol>
                    <div class="code-block python">
                        <span class="code-label">Python - Retrieval Process</span>
                        <pre>from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def find_similar_items(query_image, dataset_df, top_k=5):
    """
    Find most similar items to the query image
    """
    # Encode query image
    query_embedding = encode_image(query_image)
    
    # Convert all dataset embeddings to array
    dataset_embeddings = np.vstack(dataset_df['embedding'].values)
    
    # Calculate similarities
    similarities = cosine_similarity(
        query_embedding.reshape(1, -1),
        dataset_embeddings
    )
    
    # Get top-k indices
    top_indices = np.argsort(similarities[0])[-top_k:][::-1]
    top_scores = similarities[0][top_indices]
    
    # Retrieve metadata
    results = dataset_df.iloc[top_indices].copy()
    results['similarity_score'] = top_scores
    
    return results.sort_values('similarity_score', ascending=False)</pre>
                    </div>
                </div>

                <div class="step card">
                    <h4>Step 3: Augmentation</h4>
                    <p>
                        <strong>Objective:</strong> Combine the retrieved data with the original query to create 
                        a rich context for the LLM.
                    </p>
                    <p><strong>Process:</strong></p>
                    <ol>
                        <li>
                            <strong>Context Formatting:</strong> Organize retrieved data in a structured format 
                            (markdown, JSON, plain text).
                        </li>
                        <li>
                            <strong>Prompt Construction:</strong> Build a system prompt + user query + context.
                        </li>
                        <li>
                            <strong>Multimodal Combination:</strong> Include both visual (encoded images) and 
                            textual context.
                        </li>
                        <li>
                            <strong>Quality Assurance:</strong> Ensure context is relevant and non-contradictory.
                        </li>
                    </ol>
                    <div class="code-block python">
                        <span class="code-label">Python - Augmentation Process</span>
                        <pre>def augment_query(user_query, user_image, retrieved_items):
    """
    Combine query with retrieved data
    """
    # Format retrieved items as context
    context = "RELATED ITEMS FROM DATABASE:\n"
    for idx, item in retrieved_items.iterrows():
        context += f"""
- Item: {item['name']}
  Price: ${item['price']}
  Color: {item['color']}
  Material: {item['material']}
  Link: {item['product_link']}
"""
    
    # Build augmented prompt
    augmented_prompt = f"""You are a professional fashion analyst.
Analyze the provided image and the related items below.
Provide detailed fashion recommendations.

{context}

User Query: {user_query}

Please provide:
1. Item analysis from the image
2. Style recommendations
3. Similar items from our catalog
4. Price comparisons"""
    
    return augmented_prompt</pre>
                    </div>
                </div>

                <div class="step card">
                    <h4>Step 4: Response Generation</h4>
                    <p>
                        <strong>Objective:</strong> Use the augmented query to generate comprehensive, 
                        accurate, and contextually relevant responses.
                    </p>
                    <p><strong>Process:</strong></p>
                    <ol>
                        <li>
                            <strong>Model Initialization:</strong> Load the multimodal LLM (Llama Vision, GPT-4V, etc.).
                        </li>
                        <li>
                            <strong>Message Construction:</strong> Create the message object with text and images.
                        </li>
                        <li>
                            <strong>Inference:</strong> Send to model for generation.
                        </li>
                        <li>
                            <strong>Post-processing:</strong> Format and clean the response.
                        </li>
                    </ol>
                    <div class="code-block python">
                        <span class="code-label">Python - Generation Process</span>
                        <pre>from ibm_watsonx_ai import Credentials, APIClient
from ibm_watsonx_ai.foundation_models import ModelInference

def generate_response(augmented_prompt, image_base64):
    """
    Generate response using multimodal LLM
    """
    # Initialize credentials and client
    credentials = Credentials(
        url="https://us-south.ml.cloud.ibm.com"
    )
    client = APIClient(credentials)
    
    # Initialize model
    model = ModelInference(
        model_id="meta-llama/llama-3-2-90b-vision-instruct",
        credentials=credentials,
        project_id="your-project-id"
    )
    
    # Create messages
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": augmented_prompt
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{image_base64}"
                    }
                }
            ]
        }
    ]
    
    # Generate response
    response = model.chat(messages=messages)
    return response['choices'][0]['message']['content']</pre>
                    </div>
                </div>
            </div>
        </section>

        <!-- MULTIMODAL CHATBOTS -->
        <section id="multimodal-chatbots">
            <h2>üí¨ Multimodal Chatbots & QA Systems</h2>

            <div class="diagram-container">
                <div class="diagram-title">Multimodal AI Chatbot Architecture</div>
                <img src="https://raw.githubusercontent.com/md-sahil-analyst22/Study_Material/main/images/multimodel_ai_chatbot.png" alt="Multimodal AI Chatbot Diagram">
            </div>

            <div class="card">
                <h3>üì± What Are Multimodal Chatbots?</h3>
                <p>
                    Multimodal chatbots are advanced AI systems that can process and respond to multiple types 
                    of input data simultaneously. Unlike traditional chatbots that only understand text, multimodal 
                    chatbots can:
                </p>
                <ul>
                    <li>üëÄ <strong>See:</strong> Understand and analyze images and videos</li>
                    <li>üëÇ <strong>Hear:</strong> Process audio and speech</li>
                    <li>üìñ <strong>Read:</strong> Understand text and documents</li>
                    <li>üß† <strong>Reason:</strong> Integrate all modalities for comprehensive understanding</li>
                </ul>
            </div>

            <h3>üéØ Key Characteristics</h3>
            <div class="grid">
                <div class="feature-box">
                    <div class="feature-icon">üîå</div>
                    <div class="feature-content">
                        <h4>Multiple Input Modalities</h4>
                        <p>
                            Accept text queries, image uploads, audio inputs, and video files. 
                            Users can mix modalities in a single query.
                        </p>
                    </div>
                </div>

                <div class="feature-box">
                    <div class="feature-icon">üîó</div>
                    <div class="feature-content">
                        <h4>Integrated Understanding</h4>
                        <p>
                            The system understands relationships between modalities. 
                            A red dress image and "red dress" text are recognized as the same concept.
                        </p>
                    </div>
                </div>

                <div class="feature-box">
                    <div class="feature-icon">üìù</div>
                    <div class="feature-content">
                        <h4>Contextual Response</h4>
                        <p>
                            Responses combine information from all input modalities, providing comprehensive 
                            and contextually appropriate answers.
                        </p>
                    </div>
                </div>

                <div class="feature-box">
                    <div class="feature-icon">üó£Ô∏è</div>
                    <div class="feature-content">
                        <h4>Natural Interaction</h4>
                        <p>
                            Users interact naturally without needing to format data in specific ways. 
                            The system handles the complexity internally.
                        </p>
                    </div>
                </div>
            </div>

            <h3>üõ†Ô∏è Implementation Steps</h3>
            <div class="card">
                <p><strong>Building a multimodal QA system involves 4 core steps:</strong></p>
            </div>

            <div class="steps-container">
                <div class="step card">
                    <h4>1. Environment Setup</h4>
                    <div class="code-block python">
                        <span class="code-label">Bash</span>
                        <pre># Create virtual environment
python3.11 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install required packages
pip install \
    ibm-watsonx-ai==1.1.20 \
    torch \
    torchvision \
    pillow \
    requests \
    scikit-learn \
    flask \
    gradio</pre>
                    </div>
                </div>

                <div class="step card">
                    <h4>2. Model Initialization</h4>
                    <div class="code-block python">
                        <span class="code-label">Python</span>
                        <pre>from ibm_watsonx_ai import Credentials, APIClient
from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.foundation_models.schema import TextChatParameters

# Set up credentials
credentials = Credentials(
    url="https://us-south.ml.cloud.ibm.com"
    # API key optional in lab environments
)

# Initialize client
client = APIClient(credentials)

# Configure parameters
params = TextChatParameters(
    temperature=0.2,  # Lower = more deterministic
    top_p=0.6,        # Nucleus sampling
    max_tokens=2000
)

# Load model
model = ModelInference(
    model_id="meta-llama/llama-3-2-90b-vision-instruct",
    credentials=credentials,
    project_id="skills-network",
    params=params
)</pre>
                    </div>
                </div>

                <div class="step card">
                    <h4>3. Image Preparation</h4>
                    <div class="code-block python">
                        <span class="code-label">Python</span>
                        <pre>import base64
from io import BytesIO
from PIL import Image
import requests

class ImageProcessor:
    @staticmethod
    def encode_image_from_url(image_url):
        """Convert image URL to base64"""
        response = requests.get(image_url)
        image = Image.open(BytesIO(response.content))
        return ImageProcessor.image_to_base64(image)
    
    @staticmethod
    def encode_image_from_file(file_path):
        """Convert local image file to base64"""
        image = Image.open(file_path)
        return ImageProcessor.image_to_base64(image)
    
    @staticmethod
    def image_to_base64(image):
        """Convert PIL Image to base64 string"""
        buffered = BytesIO()
        image.save(buffered, format="JPEG")
        img_base64 = base64.b64encode(
            buffered.getvalue()
        ).decode("utf-8")
        return img_base64</pre>
                    </div>
                </div>

                <div class="step card">
                    <h4>4. Multimodal Query Function</h4>
                    <div class="code-block python">
                        <span class="code-label">Python</span>
                        <pre>def multimodal_query(image_base64, text_query, system_prompt=None):
    """
    Send image and text to model and get response
    """
    default_prompt = (
        "You are a helpful AI assistant that can analyze "
        "images and answer questions about them."
    )
    
    prompt = system_prompt or default_prompt
    
    # Construct message with both text and image
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": f"{prompt}\n\nUser query: {text_query}"
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{image_base64}"
                    }
                }
            ]
        }
    ]
    
    # Get response from model
    try:
        response = model.chat(messages=messages)
        return response['choices'][0]['message']['content']
    except Exception as e:
        return f"Error: {str(e)}"

# Example usage
image_url = "https://example.com/fashion_item.jpg"
encoded = ImageProcessor.encode_image_from_url(image_url)

response = multimodal_query(
    image_base64=encoded,
    text_query="What style is this outfit? What's the color?",
    system_prompt="You are a professional fashion consultant."
)

print(response)</pre>
                    </div>
                </div>
            </div>

            <h3>üéì Practical Examples</h3>

            <div class="accordion">
                <div class="accordion-item">
                    <button class="accordion-header" onclick="toggleAccordion(this)">
                        <span>üì∏ Example 1: Fashion Image Analysis</span>
                        <span>‚ñº</span>
                    </button>
                    <div class="accordion-content">
                        <div class="code-block python">
                            <span class="code-label">Python</span>
                            <pre>def analyze_fashion_image(image_path):
    """Analyze fashion item in image"""
    
    # Encode image
    image_base64 = ImageProcessor.encode_image_from_file(image_path)
    
    # Fashion-specific prompt
    prompt = """You are a professional fashion consultant.
Analyze the fashion item in this image and provide:
1. Item identification (type, category)
2. Color and material analysis
3. Style classification (casual, formal, business, etc.)
4. Occasion recommendations
5. Complementary styles"""
    
    response = multimodal_query(
        image_base64=image_base64,
        text_query="Analyze this fashion item",
        system_prompt=prompt
    )
    
    return response

# Usage
result = analyze_fashion_image('red_dress.jpg')
print(result)</pre>
                        </div>
                    </div>
                </div>

                <div class="accordion-item">
                    <button class="accordion-header" onclick="toggleAccordion(this)">
                        <span>üçΩÔ∏è Example 2: Nutritional Analysis</span>
                        <span>‚ñº</span>
                    </button>
                    <div class="accordion-content">
                        <div class="code-block python">
                            <span class="code-label">Python</span>
                            <pre>def analyze_meal_nutrition(image_path):
    """Estimate nutrition from food image"""
    
    image_base64 = ImageProcessor.encode_image_from_file(image_path)
    
    nutrition_prompt = """You are an expert nutritionist.
Analyze this food image and provide:
1. **Identification**: List each food item
2. **Portion Estimation**: Estimated portion sizes
3. **Calorie Count**: Total estimated calories
4. **Macronutrient Breakdown**: Protein, carbs, fats
5. **Micronutrients**: Key vitamins and minerals
6. **Health Assessment**: Is this meal healthy?
7. **Recommendations**: How to improve nutritionally"""
    
    response = multimodal_query(
        image_base64=image_base64,
        text_query="What's the nutritional content?",
        system_prompt=nutrition_prompt
    )
    
    return response

# Usage
result = analyze_meal_nutrition('lunch_plate.jpg')
print(result)</pre>
                        </div>
                    </div>
                </div>

                <div class="accordion-item">
                    <button class="accordion-header" onclick="toggleAccordion(this)">
                        <span>üîç Example 3: Medical Image Analysis</span>
                        <span>‚ñº</span>
                    </button>
                    <div class="accordion-content">
                        <div class="code-block python">
                            <span class="code-label">Python</span>
                            <pre>def analyze_medical_image(image_path, query):
    """Support medical professionals with image analysis"""
    
    image_base64 = ImageProcessor.encode_image_from_file(image_path)
    
    medical_prompt = """You are a medical imaging assistant.
Analyze this medical image and provide:
1. **Observations**: Key visual features
2. **Anatomy**: Identified anatomical structures
3. **Abnormalities**: Any notable findings (if visible)
4. **Technical Quality**: Image quality assessment
5. **Context**: Relevant clinical information

IMPORTANT: This is for educational/reference only.
Always recommend professional medical consultation."""
    
    response = multimodal_query(
        image_base64=image_base64,
        text_query=query,
        system_prompt=medical_prompt
    )
    
    return response

# Usage
result = analyze_medical_image('xray.jpg', 'What do you observe?')
print(result)</pre>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- REAL-WORLD EXAMPLES -->
        <section id="examples">
            <h2>üåç Real-World Examples</h2>

            <h3>1Ô∏è‚É£ Fashion E-commerce: Style Finder</h3>
            <div class="card">
                <h4>The Problem</h4>
                <p>
                    An online fashion retailer wants to help customers find similar items to an outfit they like. 
                    Users can upload a photo of an outfit they love, and the system should identify similar items 
                    from their inventory with pricing and availability.
                </p>
                <h4>The Solution: MM-RAG Style Finder</h4>
                <div class="content-split">
                    <div>
                        <ul>
                            <li><strong>Data Indexing:</strong> 10,000+ fashion items from their catalog are 
                            converted to embeddings using ResNet50</li>
                            <li><strong>Retrieval:</strong> User uploads an outfit photo ‚Üí converts to embedding ‚Üí 
                            finds 5 most similar items</li>
                            <li><strong>Augmentation:</strong> Retrieved items' data (price, color, material, links) 
                            combined with image</li>
                            <li><strong>Generation:</strong> Llama Vision analyzes the outfit and provides recommendations 
                            with links to similar items</li>
                        </ul>
                    </div>
                    <div class="code-block python">
                        <span class="code-label">Python</span>
                        <pre># Style Finder Implementation
class StyleFinder:
    def __init__(self, catalog_df):
        self.catalog = catalog_df
        self.processor = ImageProcessor()
        self.model = initialize_llama_model()
        
    def find_similar_styles(self, user_image):
        """Find similar fashion items"""
        # 1. Encode user image
        embedding = self.processor.encode(user_image)
        
        # 2. Find similar items
        similar = find_similar(
            embedding,
            self.catalog['embeddings'],
            top_k=5
        )
        
        # 3. Prepare context
        context = format_items(similar, self.catalog)
        
        # 4. Generate response
        response = self.model.chat({
            'image': user_image,
            'context': context,
            'query': 'Find similar styles'
        })
        
        return response

# Usage
finder = StyleFinder(fashion_catalog)
recommendations = finder.find_similar_styles(
    user_uploaded_image
)
print(recommendations)</pre>
                    </div>
                </div>
                <div class="highlight-box">
                    <strong>Impact:</strong> 45% increase in cross-selling, improved customer engagement, 
                    reduced return rates by 20%
                </div>
            </div>

            <h3>2Ô∏è‚É£ Healthcare: AI Nutrition Coach</h3>
            <div class="card">
                <h4>The Problem</h4>
                <p>
                    A health tech company wants to help users track their nutrition. Users can take photos of meals, 
                    and the AI should estimate nutritional content and provide personalized dietary recommendations 
                    based on their health goals.
                </p>
                <h4>The Solution: MM-RAG Nutrition Coach</h4>
                <div class="content-split">
                    <div>
                        <ul>
                            <li><strong>Data Indexing:</strong> Comprehensive food database with nutritional facts, 
                            portions, and preparation methods indexed</li>
                            <li><strong>Retrieval:</strong> Food image ‚Üí identify items ‚Üí retrieve nutritional data 
                            for matched foods</li>
                            <li><strong>Augmentation:</strong> Retrieved nutritional data + user health profile + 
                            dietary goals</li>
                            <li><strong>Generation:</strong> LLM provides calorie estimates, macros, and personalized 
                            recommendations</li>
                        </ul>
                    </div>
                    <div class="code-block python">
                        <span class="code-label">Python</span>
                        <pre>class NutritionCoach:
    def __init__(self, food_db):
        self.food_database = food_db
        self.user_profile = {}
        
    def analyze_meal(self, meal_image, user_id):
        """Analyze meal and provide recommendations"""
        
        # Load user profile
        self.user_profile = get_user_profile(user_id)
        
        # Encode meal image
        meal_embedding = encode_image(meal_image)
        
        # Retrieve similar foods from database
        identified_foods = retrieve_foods(
            meal_embedding,
            self.food_database,
            threshold=0.8
        )
        
        # Get nutritional data
        nutrition_data = aggregate_nutrition(
            identified_foods
        )
        
        # Generate personalized advice
        prompt = f"""User profile: {self.user_profile}
Meal analysis: {nutrition_data}
Provide personalized dietary advice"""
        
        response = generate_response(
            image=meal_image,
            prompt=prompt
        )
        
        return response

# Usage
coach = NutritionCoach(food_database)
advice = coach.analyze_meal(meal_photo, user_id=123)</pre>
                    </div>
                </div>
                <div class="highlight-box">
                    <strong>Impact:</strong> Users lose an average of 8 lbs in 8 weeks, 
                    85% user retention rate
                </div>
            </div>

            <h3>3Ô∏è‚É£ Retail: Smart Product Search</h3>
            <div class="card">
                <h4>The Problem</h4>
                <p>
                    An electronics retailer struggles with traditional search. Text-based queries don't always 
                    capture what customers want. They want a "visual search" that understands product images.
                </p>
                <h4>The Solution: MM-RAG Visual Search</h4>
                <ul>
                    <li>Customers take a photo of a product they want or like</li>
                    <li>System identifies the product and finds similar items in inventory</li>
                    <li>Returns results with specifications, prices, reviews, and availability</li>
                    <li>LLM explains why these products are similar and different</li>
                </ul>
            </div>
        </section>

        <!-- IMPLEMENTATION GUIDE -->
        <section id="implementation">
            <h2>üõ†Ô∏è Complete Implementation Guide</h2>

            <h3>üìä Project Structure</h3>
            <div class="card">
                <div class="code-block python">
                    <span class="code-label">Directory Structure</span>
                    <pre>mm_rag_project/
‚îú‚îÄ‚îÄ app.py                 # Main Flask application
‚îú‚îÄ‚îÄ config.py              # Configuration settings
‚îú‚îÄ‚îÄ image_processor.py      # Image encoding and embedding
‚îú‚îÄ‚îÄ llm_service.py         # LLM initialization and queries
‚îú‚îÄ‚îÄ database.py            # Vector database operations
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ index.html        # Web interface
‚îÇ   ‚îî‚îÄ‚îÄ base.html         # Base template
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îú‚îÄ‚îÄ css/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ style.css
‚îÇ   ‚îî‚îÄ‚îÄ js/
‚îÇ       ‚îî‚îÄ‚îÄ script.js
‚îî‚îÄ‚îÄ data/
    ‚îú‚îÄ‚îÄ catalog.csv       # Product catalog
    ‚îî‚îÄ‚îÄ embeddings.pkl    # Pre-computed embeddings</pre>
                </div>
            </div>

            <h3>üöÄ Step-by-Step Implementation</h3>

            <div class="steps-container">
                <div class="step card">
                    <h4>Step 1: Configuration Setup</h4>
                    <div class="code-block python">
                        <span class="code-label">config.py</span>
                        <pre># Configuration file for MM-RAG system

# Model Settings
MODEL_ID = "meta-llama/llama-3-2-90b-vision-instruct"
PROJECT_ID = "skills-network"
WATSONX_URL = "https://us-south.ml.cloud.ibm.com"

# Image Processing
IMAGE_SIZE = (224, 224)
NORMALIZATION_MEAN = [0.485, 0.456, 0.406]
NORMALIZATION_STD = [0.229, 0.224, 0.225]

# Retrieval Settings
SIMILARITY_THRESHOLD = 0.7
TOP_K_RESULTS = 5

# Model Parameters
TEMPERATURE = 0.2
TOP_P = 0.6
MAX_TOKENS = 2000

# Database Settings
VECTOR_DB_TYPE = "faiss"  # or "pinecone", "weaviate"
DATABASE_PATH = "data/embeddings.pkl"

# Flask Settings
DEBUG = True
PORT = 5000
HOST = "0.0.0.0"</pre>
                    </div>
                </div>

                <div class="step card">
                    <h4>Step 2: Image Processing Module</h4>
                    <div class="code-block python">
                        <span class="code-label">image_processor.py</span>
                        <pre>import torch
import torchvision.transforms as transforms
from torchvision.models import resnet50
from PIL import Image
import base64
from io import BytesIO
import requests
import numpy as np
from config import IMAGE_SIZE, NORMALIZATION_MEAN, NORMALIZATION_STD

class ImageProcessor:
    def __init__(self):
        """Initialize image processor with pre-trained model"""
        # Check for GPU
        self.device = torch.device(
            "cuda" if torch.cuda.is_available() else "cpu"
        )
        
        # Load ResNet50
        self.model = resnet50(pretrained=True).to(self.device)
        self.model.eval()
        
        # Preprocessing pipeline
        self.preprocess = transforms.Compose([
            transforms.Resize(IMAGE_SIZE),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=NORMALIZATION_MEAN,
                std=NORMALIZATION_STD
            ),
        ])
    
    def encode_image(self, image_input, is_url=True):
        """
        Convert image to base64 and embedding vector
        
        Args:
            image_input: URL or file path
            is_url: Whether input is URL
            
        Returns:
            dict: {'base64': encoded_string, 'vector': embedding}
        """
        try:
            # Load image
            if is_url:
                response = requests.get(image_input)
                response.raise_for_status()
                image = Image.open(BytesIO(response.content)).convert("RGB")
            else:
                image = Image.open(image_input).convert("RGB")
            
            # Convert to Base64
            buffered = BytesIO()
            image.save(buffered, format="JPEG")
            base64_string = base64.b64encode(
                buffered.getvalue()
            ).decode("utf-8")
            
            # Extract features
            input_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                features = self.model(input_tensor)
            
            feature_vector = features.cpu().numpy().flatten()
            
            return {
                "base64": base64_string,
                "vector": feature_vector
            }
            
        except Exception as e:
            print(f"Error encoding image: {e}")
            return {"base64": None, "vector": None}</pre>
                    </div>
                </div>

                <div class="step card">
                    <h4>Step 3: LLM Service Module</h4>
                    <div class="code-block python">
                        <span class="code-label">llm_service.py</span>
                        <pre>from ibm_watsonx_ai import Credentials, APIClient
from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.foundation_models.schema import TextChatParameters
from config import (
    MODEL_ID, PROJECT_ID, WATSONX_URL,
    TEMPERATURE, TOP_P, MAX_TOKENS
)

class LLMService:
    def __init__(self):
        """Initialize connection to Llama Vision model"""
        # Set up credentials
        self.credentials = Credentials(
            url=WATSONX_URL
            # API key optional in lab environments
        )
        
        # Initialize client
        self.client = APIClient(self.credentials)
        
        # Configure parameters
        params = TextChatParameters(
            temperature=TEMPERATURE,
            top_p=TOP_P,
            max_tokens=MAX_TOKENS
        )
        
        # Load model
        self.model = ModelInference(
            model_id=MODEL_ID,
            credentials=self.credentials,
            project_id=PROJECT_ID,
            params=params
        )
    
    def generate_response(self, encoded_image, prompt):
        """
        Generate response from model using image and prompt
        
        Args:
            encoded_image: Base64 encoded image
            prompt: Text prompt for the model
            
        Returns:
            str: Model response
        """
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{encoded_image}"
                        }
                    }
                ]
            }
        ]
        
        try:
            response = self.model.chat(messages=messages)
            return response['choices'][0]['message']['content']
        except Exception as e:
            print(f"Error generating response: {e}")
            return "Error generating response"
    
    def generate_fashion_response(self, encoded_image, retrieved_items, user_query):
        """
        Generate fashion-specific response
        
        Args:
            encoded_image: User's uploaded image
            retrieved_items: Retrieved similar items from DB
            user_query: User's question
            
        Returns:
            str: Fashion analysis with recommendations
        """
        # Format retrieved items
        items_text = "SIMILAR ITEMS FROM CATALOG:\n"
        for item in retrieved_items:
            items_text += f"""
- {item['name']} (${item['price']})
  Color: {item['color']}
  Material: {item['material']}
  Link: {item['product_link']}
"""
        
        # Build specialized prompt
        prompt = f"""You are a professional fashion consultant.
        
Analyze this outfit and provide detailed recommendations.

{items_text}

User Query: {user_query}

Please provide:
1. Item identification and analysis
2. Color and style assessment  
3. Recommendations from our catalog
4. Styling tips
5. Alternative suggestions"""
        
        return self.generate_response(encoded_image, prompt)</pre>
                    </div>
                </div>

                <div class="step card">
                    <h4>Step 4: Database Operations</h4>
                    <div class="code-block python">
                        <span class="code-label">database.py</span>
                        <pre>import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from config import TOP_K_RESULTS, SIMILARITY_THRESHOLD

class VectorDatabase:
    def __init__(self, data_path):
        """Load pre-indexed data"""
        self.data = pd.read_pickle(data_path)
        self.embeddings = np.vstack(self.data['embedding'].values)
    
    def find_similar_items(self, query_vector, top_k=None):
        """
        Find k most similar items to query
        
        Args:
            query_vector: Embedding vector of query
            top_k: Number of results to return
            
        Returns:
            list: Similar items with scores
        """
        if top_k is None:
            top_k = TOP_K_RESULTS
        
        # Calculate similarities
        similarities = cosine_similarity(
            query_vector.reshape(1, -1),
            self.embeddings
        )[0]
        
        # Filter by threshold
        valid_indices = np.where(similarities >= SIMILARITY_THRESHOLD)[0]
        
        if len(valid_indices) == 0:
            # Return top result anyway
            valid_indices = np.argsort(similarities)[-top_k:][::-1]
        else:
            # Get top k from valid results
            top_indices = valid_indices[
                np.argsort(similarities[valid_indices])[-top_k:][::-1]
            ]
        
        # Build results
        results = []
        for idx in np.sort(np.argsort(similarities)[-top_k:])[::-1]:
            results.append({
                'index': idx,
                'data': self.data.iloc[idx].to_dict(),
                'similarity': float(similarities[idx])
            })
        
        return results
    
    def get_item(self, index):
        """Get item by index"""
        return self.data.iloc[index].to_dict()</pre>
                    </div>
                </div>

                <div class="step card">
                    <h4>Step 5: Flask Web Application</h4>
                    <div class="code-block python">
                        <span class="code-label">app.py</span>
                        <pre>from flask import Flask, render_template, request, jsonify
from image_processor import ImageProcessor
from llm_service import LLMService
from database import VectorDatabase
from config import DEBUG, PORT, HOST
import os

app = Flask(__name__)

# Initialize components
image_processor = ImageProcessor()
llm_service = LLMService()
vector_db = VectorDatabase('data/embeddings.pkl')

@app.route('/', methods=['GET', 'POST'])
def index():
    """Home page"""
    if request.method == 'POST':
        return process_request()
    return render_template('index.html')

def process_request():
    """Process user request"""
    try:
        # Get uploaded file
        file = request.files.get('image')
        if not file:
            return jsonify({'error': 'No image provided'}), 400
        
        # Get user query
        user_query = request.form.get('query', '')
        
        # Process image
        temp_path = 'temp_upload.jpg'
        file.save(temp_path)
        
        image_data = image_processor.encode_image(temp_path, is_url=False)
        if image_data['vector'] is None:
            return jsonify({'error': 'Could not process image'}), 400
        
        # Find similar items
        similar_items = vector_db.find_similar_items(
            image_data['vector']
        )
        
        # Generate response
        response_text = llm_service.generate_fashion_response(
            image_data['base64'],
            [item['data'] for item in similar_items],
            user_query
        )
        
        # Clean up
        os.remove(temp_path)
        
        return jsonify({
            'success': True,
            'response': response_text,
            'similar_items': similar_items[:3]  # Return top 3
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({'status': 'healthy'})

if __name__ == '__main__':
    app.run(
        host=HOST,
        port=PORT,
        debug=DEBUG
    )</pre>
                    </div>
                </div>

                <div class="step card">
                    <h4>Step 6: Frontend HTML Template</h4>
                    <div class="code-block html">
                        <span class="code-label">HTML</span>
                        <pre>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;MM-RAG Fashion Analyzer&lt;/title&gt;
    &lt;style&gt;
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 50px auto;
            padding: 20px;
        }
        .container {
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        input[type="file"],
        textarea,
        button {
            width: 100%;
            padding: 10px;
            margin: 10px 0;
            border: 1px solid #ddd;
            border-radius: 4px;
            font-size: 14px;
        }
        button {
            background: linear-gradient(135deg, #6366f1, #ec4899);
            color: white;
            border: none;
            cursor: pointer;
            font-weight: bold;
        }
        button:hover {
            opacity: 0.9;
        }
        #result {
            margin-top: 20px;
            padding: 15px;
            background: #f0f4f8;
            border-radius: 4px;
            display: none;
        }
    &lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;div class="container"&gt;
        &lt;h1&gt;üé® Fashion Style Analyzer&lt;/h1&gt;
        
        &lt;form id="uploadForm"&gt;
            &lt;label&gt;Upload Fashion Image:&lt;/label&gt;
            &lt;input type="file" name="image" id="imageInput" 
                   accept="image/*" required&gt;
            
            &lt;label&gt;Your Query:&lt;/label&gt;
            &lt;textarea name="query" id="queryInput" 
                      placeholder="What would you like to know about this outfit?"
                      rows="4"&gt;&lt;/textarea&gt;
            
            &lt;button type="submit"&gt;Analyze Style&lt;/button&gt;
        &lt;/form&gt;
        
        &lt;div id="result"&gt;
            &lt;h2&gt;Analysis Result:&lt;/h2&gt;
            &lt;div id="resultText"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
    
    &lt;script&gt;
        document.getElementById('uploadForm').addEventListener('submit', async (e) =&gt; {
            e.preventDefault();
            
            const formData = new FormData();
            formData.append('image', document.getElementById('imageInput').files[0]);
            formData.append('query', document.getElementById('queryInput').value);
            
            try {
                const response = await fetch('/', {
                    method: 'POST',
                    body: formData
                });
                
                const data = await response.json();
                
                if (data.success) {
                    document.getElementById('resultText').innerHTML = data.response;
                    document.getElementById('result').style.display = 'block';
                } else {
                    alert('Error: ' + data.error);
                }
            } catch (error) {
                alert('Error: ' + error.message);
            }
        });
    &lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</pre>
                    </div>
                </div>
            </div>
        </section>

        <!-- USE CASES -->
        <section id="usecases">
            <h2>üíº Use Cases & Applications</h2>

            <div class="grid">
                <div class="card info">
                    <h4>üëó Fashion & Retail</h4>
                    <ul>
                        <li>Visual search for similar clothing items</li>
                        <li>Style recommendations based on images</li>
                        <li>Virtual try-on assistance</li>
                        <li>Trend identification from social media images</li>
                        <li>Inventory matching and stock suggestions</li>
                    </ul>
                </div>

                <div class="card success">
                    <h4>üè• Healthcare & Wellness</h4>
                    <ul>
                        <li>Medical image analysis and interpretation</li>
                        <li>Nutrition analysis from food images</li>
                        <li>Workout form analysis and correction</li>
                        <li>Telehealth consultation support</li>
                        <li>Dietary recommendations and meal planning</li>
                    </ul>
                </div>

                <div class="card warning">
                    <h4>üè† Real Estate & Property</h4>
                    <ul>
                        <li>Property search using interior/exterior photos</li>
                        <li>Virtual tours with AI narration</li>
                        <li>Neighborhood analysis from street images</li>
                        <li>Damage assessment for insurance</li>
                        <li>Design inspiration and renovation suggestions</li>
                    </ul>
                </div>

                <div class="card">
                    <h4>üçî Food & Hospitality</h4>
                    <ul>
                        <li>Menu item identification and recommendations</li>
                        <li>Recipe suggestions from ingredient photos</li>
                        <li>Food quality assessment</li>
                        <li>Allergen detection in food images</li>
                        <li>Restaurant search based on cuisine photos</li>
                    </ul>
                </div>

                <div class="card info">
                    <h4>üöó Automotive</h4>
                    <ul>
                        <li>Vehicle identification and valuation</li>
                        <li>Damage assessment for insurance claims</li>
                        <li>Used car market analysis</li>
                        <li>Parts identification and pricing</li>
                        <li>Service recommendation based on condition</li>
                    </ul>
                </div>

                <div class="card success">
                    <h4>üì¶ E-commerce & Supply Chain</h4>
                    <ul>
                        <li>Product image search and recommendations</li>
                        <li>Counterfeit detection</li>
                        <li>Damage detection in shipments</li>
                        <li>Inventory management and tracking</li>
                        <li>Quality control inspection</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- KEY FEATURES -->
        <section id="features">
            <h2>‚ú® Key Features & Unique Selling Propositions</h2>

            <h3>üéØ Core Features</h3>
            <table>
                <thead>
                    <tr>
                        <th>Feature</th>
                        <th>Description</th>
                        <th>Business Impact</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Multimodal Processing</strong></td>
                        <td>Processes images, text, audio, and video simultaneously</td>
                        <td>Comprehensive understanding, more accurate results</td>
                    </tr>
                    <tr>
                        <td><strong>Domain-Specific Knowledge</strong></td>
                        <td>Retrieves from proprietary databases, not just general knowledge</td>
                        <td>Accurate, business-relevant answers</td>
                    </tr>
                    <tr>
                        <td><strong>Vector Similarity</strong></td>
                        <td>Uses advanced embedding techniques for semantic matching</td>
                        <td>Fast retrieval, even with millions of items</td>
                    </tr>
                    <tr>
                        <td><strong>Contextual Augmentation</strong></td>
                        <td>Combines multiple data sources in rich context</td>
                        <td>Comprehensive, nuanced responses</td>
                    </tr>
                    <tr>
                        <td><strong>Real-time Processing</strong></td>
                        <td>Sub-second response times for interactive use</td>
                        <td>Instant user feedback, better UX</td>
                    </tr>
                    <tr>
                        <td><strong>Scalability</strong></td>
                        <td>Handles millions of items and users</td>
                        <td>Grows with business without performance loss</td>
                    </tr>
                </tbody>
            </table>

            <h3>üèÜ Unique Selling Propositions</h3>
            <div class="grid">
                <div class="card success">
                    <h4>‚úÖ Advantage #1: Proprietary Data Access</h4>
                    <p>
                        Unlike generic AI models, MM-RAG can access your specific business data, 
                        product catalogs, pricing, and inventory. This means <strong>accurate, 
                        business-relevant responses</strong> that reflect your actual offerings.
                    </p>
                </div>

                <div class="card success">
                    <h4>‚úÖ Advantage #2: Multi-Modal Integration</h4>
                    <p>
                        Process images AND text AND audio together. The system understands 
                        relationships between modalities, providing <strong>holistic understanding</strong> 
                        impossible with single-modality systems.
                    </p>
                </div>

                <div class="card success">
                    <h4>‚úÖ Advantage #3: Vector-Based Retrieval</h4>
                    <p>
                        Semantic similarity matching (not keyword matching). Find truly similar items 
                        based on meaning, not just matching words. <strong>Dramatically improves 
                        relevance</strong> of results.
                    </p>
                </div>

                <div class="card success">
                    <h4>‚úÖ Advantage #4: Cost-Effective</h4>
                    <p>
                        Retrieval augmentation means <strong>smaller, more efficient models</strong> 
                        can achieve results that previously required large models. <strong>Lower 
                        inference costs</strong> with better accuracy.
                    </p>
                </div>
            </div>
        </section>

        <!-- PROS AND CONS -->
        <section id="pros-cons">
            <h2>‚öñÔ∏è Pros & Cons</h2>

            <div class="pros-cons">
                <div class="pros">
                    <h4>‚úÖ Advantages</h4>
                    <ul>
                        <li><strong>Accuracy:</strong> Domain-specific knowledge improves response accuracy</li>
                        <li><strong>Relevance:</strong> Retrieval ensures responses are based on actual data</li>
                        <li><strong>Scalability:</strong> Vector databases handle millions of items efficiently</li>
                        <li><strong>Multimodality:</strong> Process multiple data types simultaneously</li>
                        <li><strong>Cost-effective:</strong> Smaller models achieve better results</li>
                        <li><strong>Transparency:</strong> Retrieved sources can be cited/explained</li>
                        <li><strong>Freshness:</strong> Can include real-time/recent data</li>
                        <li><strong>Business Control:</strong> Own your data, no reliance on external models</li>
                        <li><strong>Customization:</strong> Easily adapt to domain-specific needs</li>
                    </ul>
                </div>

                <div class="cons">
                    <h4>‚ö†Ô∏è Challenges</h4>
                    <ul>
                        <li><strong>Complexity:</strong> Multi-component system, harder to implement</li>
                        <li><strong>Data Quality:</strong> Garbage in, garbage out‚Äîquality data essential</li>
                        <li><strong>Embedding Quality:</strong> Poor embeddings lead to poor retrieval</li>
                        <li><strong>Index Maintenance:</strong> Must keep embeddings up-to-date</li>
                        <li><strong>Latency:</strong> Additional retrieval step adds processing time</li>
                        <li><strong>Storage:</strong> Vector databases require significant storage</li>
                        <li><strong>Tuning:</strong> Requires optimization for specific domains</li>
                        <li><strong>Hallucination Risk:</strong> LLM can still hallucinate despite context</li>
                        <li><strong>Privacy:</strong> Storing user data in databases requires protection</li>
                    </ul>
                </div>
            </div>

            <div class="card warning" style="margin-top: var(--spacing-lg);">
                <h4>üí° Mitigating the Cons</h4>
                <ul>
                    <li>
                        <strong>Complexity:</strong> Use managed services (Pinecone, Weaviate Cloud) 
                        to reduce operational burden
                    </li>
                    <li>
                        <strong>Data Quality:</strong> Invest in data governance, validation pipelines, 
                        regular audits
                    </li>
                    <li>
                        <strong>Embedding Quality:</strong> Use fine-tuned embeddings, domain-specific models
                    </li>
                    <li>
                        <strong>Index Maintenance:</strong> Implement automated pipelines for updating embeddings
                    </li>
                    <li>
                        <strong>Latency:</strong> Use caching, asynchronous processing, distributed systems
                    </li>
                    <li>
                        <strong>Storage:</strong> Implement data retention policies, compression techniques
                    </li>
                    <li>
                        <strong>Hallucination:</strong> Use temperature control, validation against retrieved sources
                    </li>
                </ul>
            </div>
        </section>

        <!-- ALTERNATIVES -->
        <section id="alternatives">
            <h2>üîÑ Alternatives & Comparisons</h2>

            <h3>MM-RAG vs Other Approaches</h3>
            <table>
                <thead>
                    <tr>
                        <th>Approach</th>
                        <th>How It Works</th>
                        <th>Best For</th>
                        <th>Limitations</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>MM-RAG</strong></td>
                        <td>Retrieves context from vector DB + LLM generation</td>
                        <td>Domain-specific multimodal tasks</td>
                        <td>Requires quality data indexing</td>
                    </tr>
                    <tr>
                        <td><strong>Fine-tuned LLM</strong></td>
                        <td>LLM trained on specific domain data</td>
                        <td>Highly specialized domains</td>
                        <td>Expensive, requires large labeled dataset</td>
                    </tr>
                    <tr>
                        <td><strong>Prompt Engineering</strong></td>
                        <td>Crafted prompts to guide base model</td>
                        <td>Quick prototyping</td>
                        <td>Limited accuracy, not data-aware</td>
                    </tr>
                    <tr>
                        <td><strong>Vision Models (GPT-4V)</strong></td>
                        <td>Multi-task learning on images and text</td>
                        <td>General image understanding</td>
                        <td>No access to proprietary data</td>
                    </tr>
                    <tr>
                        <td><strong>Semantic Search</strong></td>
                        <td>Text-only embeddings + vector search</td>
                        <td>Document retrieval, text Q&A</td>
                        <td>No image processing capability</td>
                    </tr>
                    <tr>
                        <td><strong>Traditional Search</strong></td>
                        <td>Keyword/metadata matching</td>
                        <td>Structured data queries</td>
                        <td>Poor for semantic understanding</td>
                    </tr>
                </tbody>
            </table>

            <h3>üìä Decision Matrix</h3>
            <div class="card">
                <p><strong>Choose MM-RAG if:</strong></p>
                <ul>
                    <li>You have multimodal data (images + text)</li>
                    <li>You need domain-specific, accurate responses</li>
                    <li>You want to leverage proprietary data</li>
                    <li>You need semantic understanding (not just keyword matching)</li>
                    <li>You require fast retrieval from large datasets</li>
                    <li>You want explainability (showing sources)</li>
                </ul>
            </div>

            <div class="card">
                <p><strong>Choose Fine-tuned LLM if:</strong></p>
                <ul>
                    <li>You have massive labeled training data (100k+ examples)</li>
                    <li>You need specialized domain knowledge baked in</li>
                    <li>Budget allows for expensive training</li>
                    <li>You don't have fresh data that changes often</li>
                </ul>
            </div>

            <div class="card">
                <p><strong>Choose Prompt Engineering if:</strong></p>
                <ul>
                    <li>You need quick prototyping</li>
                    <li>You have limited data</li>
                    <li>Use case is general enough for GPT-4V</li>
                    <li>No proprietary data involved</li>
                </ul>
            </div>
        </section>

        <!-- REAL-LIFE IMPLEMENTATIONS -->
        <section id="real-life">
            <h2>üåü Real-Life Implementations & Success Stories</h2>

            <h3>Case Study 1: E-commerce Fashion Platform</h3>
            <div class="card">
                <h4>Company Profile</h4>
                <p><strong>Online fashion retailer with 50,000+ items</strong></p>
                
                <h4>Challenge</h4>
                <p>
                    Customers struggle with traditional search. They want to upload a photo of an outfit 
                    they like and find similar items from the catalog. Text-based search doesn't capture 
                    visual style effectively.
                </p>

                <h4>Solution Implemented</h4>
                <ul>
                    <li>Indexed 50,000 items using ResNet50 embeddings</li>
                    <li>Deployed MM-RAG system with Llama Vision model</li>
                    <li>Built web interface for image upload and analysis</li>
                    <li>Integrated with product database for real-time pricing</li>
                </ul>

                <h4>Results</h4>
                <ul>
                    <li>‚úÖ 45% increase in cross-selling</li>
                    <li>‚úÖ 3.2x higher click-through rate on recommendations</li>
                    <li>‚úÖ 20% reduction in return rates</li>
                    <li>‚úÖ 68% of users use visual search monthly</li>
                    <li>‚úÖ Average order value increased by 22%</li>
                </ul>

                <h4>Technical Implementation</h4>
                <div class="code-block python">
                    <span class="code-label">Python</span>
                    <pre># System metrics from production
Latency: 200-400ms per request
Throughput: 500 requests/second
Index size: 12GB (50k items √ó 2048 dims √ó 4 bytes)
Accuracy: 87% relevance (user-rated)
Cost per query: $0.002</pre>
                </div>
            </div>

            <h3>Case Study 2: Healthcare Nutrition Platform</h3>
            <div class="card">
                <h4>Company Profile</h4>
                <p><strong>Health tech startup focused on nutrition coaching</strong></p>
                
                <h4>Challenge</h4>
                <p>
                    Users found manual food logging tedious. They wanted to take photos of meals 
                    and automatically get nutritional information and personalized recommendations.
                </p>

                <h4>Solution Implemented</h4>
                <ul>
                    <li>Indexed 5,000+ food items with nutritional data</li>
                    <li>Implemented MM-RAG with food image recognition</li>
                    <li>Integrated user health profiles and dietary goals</li>
                    <li>Generated personalized dietary recommendations</li>
                </ul>

                <h4>Results</h4>
                <ul>
                    <li>‚úÖ 85% user retention rate (vs 45% industry average)</li>
                    <li>‚úÖ Users lose average 8 lbs in 8 weeks</li>
                    <li>‚úÖ 92% accuracy in food identification</li>
                    <li>‚úÖ 4.2x engagement vs traditional logging</li>
                    <li>‚úÖ Generated $2M annual revenue from nutrition coaching</li>
                </ul>
            </div>

            <h3>Case Study 3: Enterprise Supply Chain</h3>
            <div class="card">
                <h4>Company Profile</h4>
                <p><strong>Large manufacturing company with complex supply chain</strong></p>
                
                <h4>Challenge</h4>
                <p>
                    Quality control required inspection of thousands of items daily. Manual inspection 
                    was slow and error-prone. They needed automated damage detection and classification.
                </p>

                <h4>Solution Implemented</h4>
                <ul>
                    <li>Trained damage detection model on inspection images</li>
                    <li>Used MM-RAG to categorize defects and retrieve similar cases</li>
                    <li>Integrated with inventory and warranty systems</li>
                    <li>Automated decision support for claims processing</li>
                </ul>

                <h4>Results</h4>
                <ul>
                    <li>‚úÖ 10x faster inspection throughput</li>
                    <li>‚úÖ 96% accuracy in defect classification</li>
                    <li>‚úÖ $5M annual savings in claim processing</li>
                    <li>‚úÖ 40% reduction in manual review time</li>
                    <li>‚úÖ Improved consistency in defect evaluation</li>
                </ul>
            </div>
        </section>

        <!-- FUTURE TRENDS -->
        <section id="future">
            <h2>üöÄ Future Trends & Scope</h2>

            <h3>üîÆ Emerging Directions</h3>
            <div class="grid">
                <div class="card">
                    <h4>1Ô∏è‚É£ Video Understanding</h4>
                    <p>
                        Beyond images, processing temporal sequences (video) with semantic understanding. 
                        MM-RAG systems will understand video context for real-time analysis.
                    </p>
                    <div class="badge badge-primary">Future: 2024-2025</div>
                </div>

                <div class="card">
                    <h4>2Ô∏è‚É£ Audio & Speech Integration</h4>
                    <p>
                        Better audio processing with emotional understanding. Spoken queries with 
                        visual context for truly natural interaction.
                    </p>
                    <div class="badge badge-secondary">Future: 2024-2025</div>
                </div>

                <div class="card">
                    <h4>3Ô∏è‚É£ 3D & Spatial Understanding</h4>
                    <p>
                        Processing 3D models, point clouds, and spatial relationships. 
                        Applications in AR/VR and architectural design.
                    </p>
                    <div class="badge badge-warning">Future: 2025-2026</div>
                </div>

                <div class="card">
                    <h4>4Ô∏è‚É£ Real-time Adaptation</h4>
                    <p>
                        Systems that learn and adapt from user feedback in real-time without retraining. 
                        Continuous improvement during operation.
                    </p>
                    <div class="badge badge-primary">Future: 2025-2026</div>
                </div>

                <div class="card">
                    <h4>5Ô∏è‚É£ Federated Learning</h4>
                    <p>
                        Privacy-preserving MM-RAG systems that don't require sending data to central servers. 
                        On-device processing with distributed learning.
                    </p>
                    <div class="badge badge-secondary">Future: 2026-2027</div>
                </div>

                <div class="card">
                    <h4>6Ô∏è‚É£ Explainability & Interpretability</h4>
                    <p>
                        Better transparency in how models make decisions. Visual explanations showing 
                        which parts of images influenced the response.
                    </p>
                    <div class="badge badge-success">Emerging Now</div>
                </div>
            </div>

            <h3>üìà Market Growth & Adoption</h3>
            <div class="card">
                <h4>Market Size & Projections</h4>
                <ul>
                    <li><strong>2024:</strong> $2.5B multimodal AI market</li>
                    <li><strong>2026:</strong> Projected $8.7B (CAGR: 86%)</li>
                    <li><strong>2030:</strong> $25B+ expected market size</li>
                </ul>

                <h4>Key Drivers</h4>
                <ul>
                    <li>Enterprise demand for AI solutions with proprietary data</li>
                    <li>Improved model capabilities (Llama 4, GPT-5, etc.)</li>
                    <li>Decline in inference costs</li>
                    <li>Better vector database solutions</li>
                    <li>Increased data collection and digitization</li>
                    <li>Regulatory push for explainability</li>
                </ul>
            </div>

            <h3>üéØ Future Applications</h3>
            <div class="card warning">
                <h4>Near Term (2024-2025)</h4>
                <ul>
                    <li>Autonomous retail checkout with image analysis</li>
                    <li>Real-time medical imaging diagnosis support</li>
                    <li>Automated quality control in manufacturing</li>
                    <li>Personalized shopping and recommendation engines</li>
                    <li>Enhanced document analysis and processing</li>
                </ul>
            </div>

            <div class="card info">
                <h4>Medium Term (2025-2027)</h4>
                <ul>
                    <li>Humanoid robots with true multimodal perception</li>
                    <li>Autonomous vehicle scene understanding</li>
                    <li>Immersive AR/VR experiences with AI guidance</li>
                    <li>Personalized medical treatment based on imaging</li>
                    <li>Intelligent security and surveillance systems</li>
                </ul>
            </div>

            <div class="card">
                <h4>Long Term (2027+)</h4>
                <ul>
                    <li>General-purpose AI systems understanding multiple modalities</li>
                    <li>Seamless human-AI collaboration in professional settings</li>
                    <li>Predictive maintenance using environmental sensing</li>
                    <li>Personalized education with adaptive multimodal content</li>
                    <li>Quantum-accelerated vector similarity search</li>
                </ul>
            </div>

            <h3>üåç Sector-Specific Growth</h3>
            <table>
                <thead>
                    <tr>
                        <th>Sector</th>
                        <th>Growth Rate</th>
                        <th>Key Applications</th>
                        <th>Timeline</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Healthcare</strong></td>
                        <td>45% CAGR</td>
                        <td>Diagnostic imaging, nutrition, treatment planning</td>
                        <td>2024-2026</td>
                    </tr>
                    <tr>
                        <td><strong>Retail/E-commerce</strong></td>
                        <td>52% CAGR</td>
                        <td>Visual search, recommendations, checkout</td>
                        <td>2024-2025</td>
                    </tr>
                    <tr>
                        <td><strong>Manufacturing</strong></td>
                        <td>38% CAGR</td>
                        <td>Quality control, defect detection, maintenance</td>
                        <td>2025-2027</td>
                    </tr>
                    <tr>
                        <td><strong>Automotive</strong></td>
                        <td>41% CAGR</td>
                        <td>Autonomous driving, damage assessment</td>
                        <td>2025-2027</td>
                    </tr>
                    <tr>
                        <td><strong>Real Estate</strong></td>
                        <td>33% CAGR</td>
                        <td>Property search, virtual tours, valuation</td>
                        <td>2024-2026</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- CONCLUSION -->
        <section>
            <div class="card success">
                <h2>üéì Conclusion: The MM-RAG Revolution</h2>
                <p>
                    Multimodal Retrieval-Augmented Generation (MM-RAG) represents a fundamental shift 
                    in how AI systems can be built and deployed. By combining the power of large language 
                    models with domain-specific data and multimodal processing, MM-RAG enables:
                </p>
                
                <h3>Key Takeaways</h3>
                <ul>
                    <li>
                        <strong>Accuracy:</strong> Domain-specific knowledge vastly improves response quality
                    </li>
                    <li>
                        <strong>Multimodality:</strong> Processing images, text, audio together provides 
                        comprehensive understanding
                    </li>
                    <li>
                        <strong>Relevance:</strong> Retrieved sources ensure responses are grounded in reality, 
                        not hallucinations
                    </li>
                    <li>
                        <strong>Scalability:</strong> Vector databases efficiently handle millions of items 
                        and queries
                    </li>
                    <li>
                        <strong>Customization:</strong> Easy to adapt to specific business domains and use cases
                    </li>
                    <li>
                        <strong>Cost-Effectiveness:</strong> Smaller models achieve better results through retrieval 
                        augmentation
                    </li>
                </ul>

                <h3>The Path Forward</h3>
                <p>
                    As AI continues to evolve, MM-RAG systems will become increasingly important in 
                    enterprise applications. Organizations that master this technology now will have a 
                    significant competitive advantage. The combination of:
                </p>
                <ul>
                    <li>Improved model capabilities</li>
                    <li>Better vector database infrastructure</li>
                    <li>Decreasing computational costs</li>
                    <li>Increasing organizational data availability</li>
                </ul>
                <p>
                    ...creates an unprecedented opportunity for AI-driven transformation across industries.
                </p>

                <div class="highlight-box">
                    <strong>Final Thought:</strong> MM-RAG isn't just another AI technique‚Äîit's a 
                    foundational approach that will shape how enterprises build intelligent systems 
                    for the next decade. Whether you're building fashion recommendation systems, 
                    healthcare platforms, or supply chain optimizations, MM-RAG provides a proven 
                    framework for success.
                </div>
            </div>
        </section>
    </div>

    <!-- Footer -->
    <footer>
        <p><strong>üìö Comprehensive MM-RAG & Multimodal AI Study Guide</strong></p>
        <p>Complete resource covering concepts, architecture, implementation, and real-world applications</p>
        <p style="margin-top: 20px; opacity: 0.9;">
            Last Updated: December 2025 | 
            <a href="#introduction" style="color: white; text-decoration: underline;">Back to Top</a>
        </p>
        <p style="font-size: 0.9rem; margin-top: 10px;">
            &copy; 2025 AI Education Series. All rights reserved.
        </p>
    </footer>

    <!-- Back to Top Button -->
    <button class="back-to-top" id="backToTop" onclick="scrollToTop()">‚Üë</button>

    <!-- Scripts -->
    <script>
        // Back to Top functionality
        const backToTopBtn = document.getElementById('backToTop');

        window.addEventListener('scroll', () => {
            if (window.pageYOffset > 300) {
                backToTopBtn.classList.add('show');
            } else {
                backToTopBtn.classList.remove('show');
            }
        });

        function scrollToTop() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        }

        // Navigation toggle (for mobile)
        const navToggle = document.querySelector('.nav-toggle');
        const navMenu = document.getElementById('navMenu');

        if (navToggle) {
            navToggle.addEventListener('click', () => {
                navMenu.style.display = navMenu.style.display === 'flex' ? 'none' : 'flex';
            });
        }

        // Accordion functionality
        function toggleAccordion(header) {
            const content = header.nextElementSibling;
            header.classList.toggle('active');
            content.classList.toggle('show');
            
            // Close other open accordions
            document.querySelectorAll('.accordion-header').forEach(h => {
                if (h !== header && h.classList.contains('active')) {
                    h.classList.remove('active');
                    h.nextElementSibling.classList.remove('show');
                }
            });
        }

        // Smooth scroll for navigation links
        document.querySelectorAll('.toc a, .nav-menu a').forEach(link => {
            link.addEventListener('click', (e) => {
                const href = link.getAttribute('href');
                if (href.startsWith('#')) {
                    e.preventDefault();
                    const target = document.querySelector(href);
                    if (target) {
                        target.scrollIntoView({ behavior: 'smooth', block: 'start' });
                    }
                }
            });
        });

        // Add animation on scroll
        const observerOptions = {
            threshold: 0.1,
            rootMargin: '0px 0px -100px 0px'
        };

        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.style.opacity = '1';
                }
            });
        }, observerOptions);

        document.querySelectorAll('.card, section').forEach(el => {
            observer.observe(el);
        });

        // Print-friendly styles
        window.addEventListener('beforeprint', () => {
            document.body.style.backgroundColor = 'white';
            document.querySelectorAll('.card').forEach(card => {
                card.style.pageBreakInside = 'avoid';
            });
        });

        console.log('%cüìö MM-RAG Study Guide loaded successfully!', 'color: #6366f1; font-size: 16px; font-weight: bold;');
        console.log('%cTotal sections: 15 | Estimated reading time: 45-60 minutes', 'color: #ec4899;');
    </script>
</body>
</html>
