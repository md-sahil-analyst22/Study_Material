<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Complete Guide to LlamaIndex & RAG Applications</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
.infographic-section {
    background: var(--card-bg);
    padding: 2rem;
    margin: 2rem 0;
    border-radius: 12px;
    text-align: center;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
}

.infographic-section img {
    max-width: 100%;
    height: auto;
    border-radius: 8px;
    display: inline-block;
}


        :root {
            --primary-color: #2563eb;
            --secondary-color: #7c3aed;
            --accent-color: #f59e0b;
            --success-color: #10b981;
            --danger-color: #ef4444;
            --dark-bg: #1e293b;
            --light-bg: #f8fafc;
            --card-bg: #ffffff;
            --text-primary: #1e293b;
            --text-secondary: #64748b;
            --border-color: #e2e8f0;
            --gradient-1: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            --gradient-2: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            --gradient-3: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
            --gradient-4: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: var(--text-primary);
            background: var(--light-bg);
            overflow-x: hidden;
        }

        /* Header Styles */
        header {
            background: var(--gradient-1);
            color: white;
            padding: 3rem 2rem;
            text-align: center;
            position: relative;
            overflow: hidden;
        }

        header::before {
            content: '';
            position: absolute;
            top: -50%;
            left: -50%;
            width: 200%;
            height: 200%;
            background: radial-gradient(circle, rgba(255,255,255,0.1) 1px, transparent 1px);
            background-size: 50px 50px;
            animation: moveBackground 20s linear infinite;
        }

        @keyframes moveBackground {
            0% { transform: translate(0, 0); }
            100% { transform: translate(50px, 50px); }
        }

        header h1 {
            font-size: clamp(2rem, 5vw, 3.5rem);
            margin-bottom: 1rem;
            position: relative;
            z-index: 1;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
        }

        header p {
            font-size: clamp(1rem, 2vw, 1.25rem);
            opacity: 0.95;
            position: relative;
            z-index: 1;
        }

        /* Navigation */
        nav {
            background: var(--card-bg);
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            position: sticky;
            top: 0;
            z-index: 1000;
            padding: 1rem 2rem;
        }

        nav ul {
            list-style: none;
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 1rem;
        }

        nav a {
            text-decoration: none;
            color: var(--text-primary);
            padding: 0.5rem 1rem;
            border-radius: 8px;
            transition: all 0.3s ease;
            font-weight: 500;
        }

        nav a:hover {
            background: var(--gradient-1);
            color: white;
            transform: translateY(-2px);
        }

        /* Container */
        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
        }

        /* Section Styles */
        section {
            margin: 4rem 0;
            animation: fadeInUp 0.8s ease-out;
        }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(30px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        h2 {
            font-size: clamp(1.75rem, 4vw, 2.5rem);
            margin-bottom: 1.5rem;
            color: var(--primary-color);
            position: relative;
            padding-bottom: 0.5rem;
        }

        h2::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 0;
            width: 60px;
            height: 4px;
            background: var(--gradient-1);
            border-radius: 2px;
        }

        h3 {
            font-size: clamp(1.25rem, 3vw, 1.75rem);
            margin: 2rem 0 1rem;
            color: var(--secondary-color);
        }

        /* Card Styles */
        .card {
            background: var(--card-bg);
            border-radius: 12px;
            padding: 2rem;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            margin: 1.5rem 0;
            transition: all 0.3s ease;
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 20px rgba(0,0,0,0.15);
        }

        /* Grid Layouts */
        .grid {
            display: grid;
            gap: 2rem;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
        }

        .grid-2 {
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
        }

        /* Use Cases Grid - Responsive Layout */
        #use-cases .grid {
            grid-template-columns: 1fr;
        }

        @media (min-width: 768px) {
            #use-cases .grid {
                grid-template-columns: repeat(2, 1fr);
            }
        }

        @media (min-width: 1024px) {
            #use-cases .grid {
                grid-template-columns: repeat(2, 1fr);
            }
        }

        /* Feature Cards */
        .feature-card {
            background: var(--card-bg);
            border-radius: 12px;
            padding: 2rem;
            text-align: center;
            transition: all 0.3s ease;
            border: 2px solid transparent;
        }

        .feature-card:hover {
            border-color: var(--primary-color);
            transform: scale(1.05);
        }

        .feature-icon {
            font-size: 3rem;
            margin-bottom: 1rem;
        }

        /* Code Blocks */
        .code-block {
            background: var(--dark-bg);
            color: #f8f8f2;
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1rem 0;
            position: relative;
        }

        .code-block::before {
            content: attr(data-lang);
            position: absolute;
            top: 0.5rem;
            right: 0.5rem;
            background: var(--primary-color);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 4px;
            font-size: 0.75rem;
            font-weight: bold;
        }

        pre {
            margin: 0;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.5;
        }

        code {
            font-family: 'Courier New', monospace;
        }

        /* Info Boxes */
        .info-box {
            padding: 1.5rem;
            border-radius: 8px;
            margin: 1.5rem 0;
            border-left: 4px solid;
        }

        .info-box.info {
            background: #dbeafe;
            border-color: var(--primary-color);
        }

        .info-box.warning {
            background: #fef3c7;
            border-color: var(--accent-color);
        }

        .info-box.success {
            background: #d1fae5;
            border-color: var(--success-color);
        }

        .info-box.danger {
            background: #fee2e2;
            border-color: var(--danger-color);
        }

        /* Tables */
        .table-container {
            overflow-x: auto;
            margin: 1.5rem 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            background: var(--card-bg);
            border-radius: 8px;
            overflow: hidden;
        }

        th {
            background: var(--gradient-1);
            color: white;
            padding: 1rem;
            text-align: left;
            font-weight: 600;
        }

        td {
            padding: 1rem;
            border-bottom: 1px solid var(--border-color);
        }

        tr:hover {
            background: var(--light-bg);
        }

        /* Comparison Table */
        .comparison-table {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1rem;
            margin: 2rem 0;
        }

        .comparison-item {
            background: var(--card-bg);
            padding: 1.5rem;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .comparison-item h4 {
            color: var(--primary-color);
            margin-bottom: 1rem;
        }

        /* Badges */
        .badge {
            display: inline-block;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            margin: 0.25rem;
        }

        .badge-primary { background: var(--primary-color); color: white; }
        .badge-secondary { background: var(--secondary-color); color: white; }
        .badge-success { background: var(--success-color); color: white; }
        .badge-warning { background: var(--accent-color); color: white; }

        /* Timeline */
        .timeline {
            position: relative;
            padding: 2rem 0;
        }

        .timeline::before {
            content: '';
            position: absolute;
            left: 50%;
            top: 0;
            bottom: 0;
            width: 4px;
            background: var(--gradient-1);
            transform: translateX(-50%);
        }

        .timeline-item {
            position: relative;
            margin: 2rem 0;
            padding: 0 2rem;
        }

        .timeline-item::before {
            content: '';
            position: absolute;
            left: 50%;
            width: 20px;
            height: 20px;
            background: var(--primary-color);
            border-radius: 50%;
            transform: translateX(-50%);
            border: 4px solid white;
            box-shadow: 0 0 0 4px var(--primary-color);
        }

        .timeline-content {
            background: var(--card-bg);
            padding: 1.5rem;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            width: calc(50% - 2rem);
        }

        .timeline-item:nth-child(odd) .timeline-content {
            margin-left: auto;
        }

        /* Mermaid Diagrams */
        .mermaid {
            text-align: center;
            margin: 2rem 0;
            background: white;
            padding: 2rem;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        /* Pros/Cons Lists */
        .pros-cons {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2rem;
            margin: 2rem 0;
        }

        .pros, .cons {
            padding: 1.5rem;
            border-radius: 8px;
        }

        .pros {
            background: #d1fae5;
            border-left: 4px solid var(--success-color);
        }

        .cons {
            background: #fee2e2;
            border-left: 4px solid var(--danger-color);
        }

        .pros h4, .cons h4 {
            margin-bottom: 1rem;
        }

        .pros ul, .cons ul {
            list-style: none;
            padding-left: 0;
        }

        .pros li::before {
            content: "‚úì ";
            color: var(--success-color);
            font-weight: bold;
            margin-right: 0.5rem;
        }

        .cons li::before {
            content: "‚úó ";
            color: var(--danger-color);
            font-weight: bold;
            margin-right: 0.5rem;
        }

        /* Footer */
        footer {
            background: var(--dark-bg);
            color: white;
            text-align: center;
            padding: 2rem;
            margin-top: 4rem;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            .timeline::before {
                left: 20px;
            }

            .timeline-item::before {
                left: 20px;
            }

            .timeline-content {
                width: calc(100% - 60px);
                margin-left: 60px !important;
            }

            nav ul {
                flex-direction: column;
                align-items: center;
            }

            .grid, .grid-2 {
                grid-template-columns: 1fr;
            }
        }

        /* Animations */
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }

        .pulse {
            animation: pulse 2s ease-in-out infinite;
        }

        /* Interactive Elements */
        .interactive-demo {
            background: var(--card-bg);
            padding: 2rem;
            border-radius: 12px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            margin: 2rem 0;
        }

        button {
            background: var(--gradient-1);
            color: white;
            border: none;
            padding: 0.75rem 1.5rem;
            border-radius: 8px;
            font-size: 1rem;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.2);
        }

        /* Highlight Box */
        .highlight-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem;
            border-radius: 12px;
            margin: 2rem 0;
            box-shadow: 0 10px 25px rgba(102, 126, 234, 0.3);
        }

        .highlight-box h3 {
            color: white;
            margin-top: 0;
        }

        /* Scrollbar Styling */
        ::-webkit-scrollbar {
            width: 10px;
            height: 10px;
        }

        ::-webkit-scrollbar-track {
            background: var(--light-bg);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--primary-color);
            border-radius: 5px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--secondary-color);
        }
    </style>
</head>
<body>
    <header>
        <h1>üöÄ Complete Guide to LlamaIndex & RAG Applications</h1>
        <p>Master Retrieval-Augmented Generation with LlamaIndex and IBM watsonx</p>
    </header>

    <nav>
        <ul>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#what-is">What is LlamaIndex?</a></li>
            <li><a href="#rag">Understanding RAG</a></li>
            <li><a href="#core-concepts">Core Concepts</a></li>
            <li><a href="#architecture">Architecture</a></li>
            <li><a href="#implementation">Implementation</a></li>
            <li><a href="#comparison">LlamaIndex vs LangChain</a></li>
            <li><a href="#use-cases">Use Cases</a></li>
            <li><a href="#pros-cons">Pros & Cons</a></li>
            <li><a href="#future">Future Trends</a></li>
        </ul>
    </nav>

    <div class="container">
        <!-- Introduction Section -->
        <section id="introduction">
            <h2>üìö Introduction</h2>
            <div class="card">
                <p>Welcome to the comprehensive guide on LlamaIndex and Retrieval-Augmented Generation (RAG) applications. This guide will take you from fundamental concepts to advanced implementation techniques, equipping you with the knowledge to build sophisticated AI-powered applications.</p>
                
                <div class="highlight-box">
                    <h3>Why This Matters</h3>
                    <p>Large Language Models (LLMs) are powerful, but they have a critical limitation: they only know what they were trained on. RAG solves this by allowing LLMs to access and reason over your specific data, creating applications that are both intelligent and contextually aware.</p>
                </div>

                <div class="info-box info">
                    <strong>üí° Key Learning Objectives:</strong>
                    <ul>
                        <li>Understand the fundamentals of RAG architecture</li>
                        <li>Master LlamaIndex components and workflow</li>
                        <li>Build production-ready RAG applications</li>
                        <li>Compare different frameworks and choose the right tool</li>
                        <li>Deploy and scale RAG systems effectively</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- What is LlamaIndex Section -->
        <section id="what-is">
            <h2>üîç What is LlamaIndex?</h2>
            
            <div class="card">
                <p><strong>LlamaIndex</strong> is an open-source data orchestration platform designed specifically for building Large Language Model (LLM) applications. It provides a comprehensive framework for context augmentation through Retrieval-Augmented Generation (RAG).</p>

                <h3>Core Definition</h3>
                <p>LlamaIndex serves as a bridge between your private data sources and LLMs, enabling applications to:</p>
                <ul>
                    <li>Ingest data from diverse sources (PDFs, databases, APIs, websites)</li>
                    <li>Structure and index data for efficient retrieval</li>
                    <li>Query the indexed data to provide context to LLMs</li>
                    <li>Generate accurate, context-aware responses</li>
                </ul>

                <div class="grid">
                    <div class="feature-card">
                        <div class="feature-icon">üì•</div>
                        <h4>Data Ingestion</h4>
                        <p>Load data from 100+ sources with native support for multiple file formats</p>
                    </div>
                    <div class="feature-card">
                        <div class="feature-icon">üî¢</div>
                        <h4>Smart Indexing</h4>
                        <p>Automatically create vector embeddings and store them efficiently</p>
                    </div>
                    <div class="feature-card">
                        <div class="feature-icon">üîé</div>
                        <h4>Semantic Search</h4>
                        <p>Retrieve the most relevant information based on meaning, not just keywords</p>
                    </div>
                    <div class="feature-card">
                        <div class="feature-icon">ü§ñ</div>
                        <h4>LLM Integration</h4>
                        <p>Seamlessly connect with multiple LLM providers including IBM watsonx, OpenAI, and more</p>
                    </div>
                </ul>
            </div>

            <div class="card">
                <h3>Why LlamaIndex?</h3>
                
                <div class="comparison-table">
                    <div class="comparison-item">
                        <h4>üéØ Simplicity</h4>
                        <p>Powerful defaults and integrated components reduce boilerplate code and speed up development</p>
                    </div>
                    <div class="comparison-item">
                        <h4>üîß Flexibility</h4>
                        <p>Modular architecture allows you to customize every component while maintaining ease of use</p>
                    </div>
                    <div class="comparison-item">
                        <h4>üì¶ Native Tools</h4>
                        <p>Built-in solutions like SimpleDirectoryReader and VectorStoreIndex handle complex tasks out-of-the-box</p>
                    </div>
                    <div class="comparison-item">
                        <h4>üöÄ Performance</h4>
                        <p>Optimized for production use with efficient indexing and retrieval mechanisms</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Understanding RAG Section -->
        <section id="rag">
            <h2>üß† Understanding Retrieval-Augmented Generation (RAG)</h2>
            
            <div class="card">
                <h3>What is RAG?</h3>
                <p>Retrieval-Augmented Generation (RAG) is a technique that enhances LLM responses by incorporating relevant external information. Instead of relying solely on training data, RAG systems retrieve pertinent information from a knowledge base and use it to generate more accurate, contextual responses.</p>

                <div class="info-box info">
                    <strong>üéØ The RAG Problem Statement:</strong>
                    <p>LLMs are trained on vast datasets, but they don't naturally include YOUR specific data. RAG bridges this gap by dynamically providing your data as context to the LLM.</p>
                </div>

                <h3>The RAG Workflow Visualized</h3>
                <div class="mermaid">
                    graph LR
                        A[User Query] --> B[Query Embedding]
                        B --> C[Vector Search]
                        C --> D[Retrieve Relevant Docs]
                        D --> E[Augment Prompt]
                        E --> F[LLM Processing]
                        F --> G[Context-Aware Response]
                        
                        style A fill:#667eea,color:#fff
                        style G fill:#43e97b,color:#fff
                        style C fill:#f093fb,color:#fff
                        style F fill:#4facfe,color:#fff
                </div>
            </div>

            <div class="card">
                <h3>The Five Stages of RAG Pipeline</h3>
                
                <div class="timeline">
                    <div class="timeline-item">
                        <div class="timeline-content">
                            <h4>1. Loading</h4>
                            <p><strong>Purpose:</strong> Ingest data from various sources</p>
                            <p><strong>Tools:</strong> SimpleDirectoryReader, LlamaHub connectors</p>
                            <p><strong>Formats:</strong> PDF, TXT, CSV, JSON, HTML, Markdown, DOCX, and more</p>
                        </div>
                    </div>
                    
                    <div class="timeline-item">
                        <div class="timeline-content">
                            <h4>2. Splitting (Chunking)</h4>
                            <p><strong>Purpose:</strong> Break documents into manageable pieces</p>
                            <p><strong>Tools:</strong> SentenceSplitter, SemanticSplitter</p>
                            <p><strong>Strategy:</strong> Balance between context and precision</p>
                        </div>
                    </div>
                    
                    <div class="timeline-item">
                        <div class="timeline-content">
                            <h4>3. Indexing</h4>
                            <p><strong>Purpose:</strong> Create vector embeddings for semantic search</p>
                            <p><strong>Tools:</strong> VectorStoreIndex, embedding models</p>
                            <p><strong>Output:</strong> Numerical representations capturing meaning</p>
                        </div>
                    </div>
                    
                    <div class="timeline-item">
                        <div class="timeline-content">
                            <h4>4. Storing</h4>
                            <p><strong>Purpose:</strong> Persist indexed data for reuse</p>
                            <p><strong>Options:</strong> In-memory, ChromaDB, Faiss, Milvus, Pinecone</p>
                            <p><strong>Benefit:</strong> Avoid re-indexing on every run</p>
                        </div>
                    </div>
                    
                    <div class="timeline-item">
                        <div class="timeline-content">
                            <h4>5. Querying</h4>
                            <p><strong>Purpose:</strong> Retrieve relevant data and generate responses</p>
                            <p><strong>Components:</strong> Retriever, Query Engine, Response Synthesizer</p>
                            <p><strong>Result:</strong> Context-aware, accurate answers</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="card">
                <h3>RAG Architecture Diagram</h3>
                <div class="mermaid">
                    graph TB
                        subgraph "Data Ingestion Layer"
                            A[PDF Files] --> D[Document Loader]
                            B[Databases] --> D
                            C[APIs/Web] --> D
                        end
                        
                        subgraph "Processing Layer"
                            D --> E[Text Splitter]
                            E --> F[Embedding Model]
                            F --> G[Vector Store]
                        end
                        
                        subgraph "Query Layer"
                            H[User Query] --> I[Query Embedding]
                            I --> J[Retriever]
                            G --> J
                            J --> K[Retrieved Context]
                        end
                        
                        subgraph "Generation Layer"
                            K --> L[Prompt Template]
                            H --> L
                            L --> M[LLM]
                            M --> N[Final Response]
                        end
                        
                        style D fill:#667eea,color:#fff
                        style G fill:#f093fb,color:#fff
                        style M fill:#43e97b,color:#fff
                        style N fill:#4facfe,color:#fff
                </div>
            </div>
        </section>

        <!-- Core Concepts Section -->
        <section id="core-concepts">
            <h2>‚öôÔ∏è Core Concepts and Components</h2>

            <div class="card">
                <h3>1. Documents and Nodes</h3>
                
                <div class="info-box info">
                    <strong>Document:</strong> A container that wraps entire documents or data from your source. It stores the text content, unique ID, metadata, and relationships to other documents.
                </div>

                <div class="info-box success">
                    <strong>Node:</strong> A chunk or piece of a Document. Nodes are the atomic units of data in LlamaIndex that get embedded and indexed. They inherit structure from Documents.
                </div>

                <div class="code-block" data-lang="Python">
<pre>from llama_index.core import Document, SimpleDirectoryReader

# Creating a Document manually
doc = Document(
    text="LlamaIndex is a framework for LLM applications.",
    metadata={"source": "documentation", "author": "AI Team"}
)

# Loading documents from a directory
documents = SimpleDirectoryReader(
    input_dir="./data",
    recursive=True,
    required_exts=[".pdf", ".txt", ".docx"]
).load_data()

print(f"Loaded {len(documents)} documents")
</pre>
                </div>
            </div>

            <div class="card">
                <h3>2. Text Splitting and Chunking</h3>
                
                <p>Text splitting is crucial for RAG performance. Smaller chunks improve retrieval precision but may lack context, while larger chunks provide more context but can introduce noise.</p>

                <div class="table-container">
                    <table>
                        <thead>
                            <tr>
                                <th>Splitter Type</th>
                                <th>Strategy</th>
                                <th>Use Case</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>SentenceSplitter</strong></td>
                                <td>Token-based, recursive character splitting</td>
                                <td>General-purpose, most common choice</td>
                            </tr>
                            <tr>
                                <td><strong>SemanticSplitter</strong></td>
                                <td>Splits based on semantic similarity thresholds</td>
                                <td>Concept-based content, articles</td>
                            </tr>
                            <tr>
                                <td><strong>CodeSplitter</strong></td>
                                <td>Language-aware code parsing</td>
                                <td>Technical documentation, code repos</td>
                            </tr>
                            <tr>
                                <td><strong>HTMLNodeParser</strong></td>
                                <td>HTML structure-based splitting</td>
                                <td>Web pages, documentation sites</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <div class="code-block" data-lang="Python">
<pre>from llama_index.core.node_parser import SentenceSplitter

# Initialize the splitter
splitter = SentenceSplitter(
    chunk_size=512,        # Maximum tokens per chunk
    chunk_overlap=50,      # Overlap between chunks
    separator=" ",         # Split on spaces
    paragraph_separator="\n\n"
)

# Split documents into nodes
nodes = splitter.get_nodes_from_documents(documents)

# Examine a node
print(f"Node text: {nodes[0].get_content()[:200]}...")
print(f"Node metadata: {nodes[0].metadata}")
</pre>
                </div>
            </div>

            <div class="card">
                <h3>3. Vector Embeddings</h3>
                
                <p>Vector embeddings are numerical representations of text that capture semantic meaning. Similar concepts have mathematically similar vectors, enabling semantic search.</p>

                <div class="highlight-box">
                    <h4>How Embeddings Work</h4>
                    <p>An embedding model transforms text into a high-dimensional vector (typically 384, 768, or 1536 dimensions). The position of these vectors in space reflects their semantic relationships.</p>
                </div>

                <div class="code-block" data-lang="Python">
<pre>from llama_index.embeddings.ibm import WatsonxEmbeddings

# Initialize embedding model
embed_model = WatsonxEmbeddings(
    model_id="ibm/slate-125m-english-rtrvr",
    url="https://us-south.ml.cloud.ibm.com",
    project_id="your-project-id"
)

# Generate embeddings
text = "LlamaIndex simplifies RAG development"
embedding = embed_model.get_text_embedding(text)

print(f"Embedding dimensions: {len(embedding)}")
print(f"First 5 values: {embedding[:5]}")
</pre>
                </div>
            </div>

            <div class="card">
                <h3>4. VectorStoreIndex</h3>
                
                <p>The VectorStoreIndex is LlamaIndex's central class for creating and managing vector databases. It combines embedding generation and storage in a single, elegant interface.</p>

                <div class="info-box success">
                    <strong>Key Advantage:</strong> VectorStoreIndex handles both embedding and storage in one command, making downstream code agnostic to the vector store backend.
                </div>

                <div class="code-block" data-lang="Python">
<parameter name="new_str"># Examine a node
print(f"Node text: {nodes[0].get_content()[:200]}...")
print(f"Node metadata: {nodes[0].metadata}")
</pre>
                </div>
            </div>

            <div class="card">
                <h3>3. Vector Embeddings</h3>
                
                <p>Vector embeddings are numerical representations of text that capture semantic meaning. Similar concepts have mathematically similar vectors, enabling semantic search.</p>

                <div class="highlight-box">
                    <h4>How Embeddings Work</h4>
                    <p>An embedding model transforms text into a high-dimensional vector (typically 384, 768, or 1536 dimensions). The position of these vectors in space reflects their semantic relationships.</p>
                </div>

                <div class="code-block" data-lang="Python">
<pre>from llama_index.embeddings.ibm import WatsonxEmbeddings

# Initialize embedding model
embed_model = WatsonxEmbeddings(
    model_id="ibm/slate-125m-english-rtrvr",
    url="https://us-south.ml.cloud.ibm.com",
    project_id="your-project-id"
)

# Generate embeddings
text = "LlamaIndex simplifies RAG development"
embedding = embed_model.get_text_embedding(text)

print(f"Embedding dimensions: {len(embedding)}")
print(f"First 5 values: {embedding[:5]}")
</pre>
                </div>
            </div>

            <div class="card">
                <h3>4. VectorStoreIndex</h3>
                
                <p>The VectorStoreIndex is LlamaIndex's central class for creating and managing vector databases. It combines embedding generation and storage in a single, elegant interface.</p>

                <div class="info-box success">
                    <strong>Key Advantage:</strong> VectorStoreIndex handles both embedding and storage in one command, making downstream code agnostic to the vector store backend.
                </div>

                <div class="code-block" data-lang="Python">
<pre>from llama_index.core import VectorStoreIndex
from llama_index.embeddings.ibm import WatsonxEmbeddings

# Create embedding model
embed_model = WatsonxEmbeddings(
    model_id="ibm/slate-125m-english-rtrvr"
)

# Create index from nodes (embedding + storage in one step!)
index = VectorStoreIndex(
    nodes=nodes,
    embed_model=embed_model,
    show_progress=True
)

# Index is now ready for querying
print("Vector index created successfully!")
</pre>
                </div>

                <h4>Integration with Vector Databases</h4>
                <div class="code-block" data-lang="Python">
<pre>import chromadb
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import StorageContext

# Initialize ChromaDB
chroma_client = chromadb.PersistentClient(path="./chroma_db")
chroma_collection = chroma_client.get_or_create_collection("my_collection")

# Create vector store
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

# Create storage context
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# Create index with persistent storage
index = VectorStoreIndex(
    nodes=nodes,
    storage_context=storage_context,
    embed_model=embed_model
)
</pre>
                </div>
            </div>

            <div class="card">
                <h3>5. Query Engines and Retrievers</h3>
                
                <p>Query engines are the interface for asking questions and getting answers. They orchestrate the retrieval and generation process.</p>

                <div class="mermaid">
                    graph LR
                        A[Query Engine] --> B[Retriever]
                        A --> C[Response Synthesizer]
                        B --> D[Vector Store]
                        D --> E[Top-K Documents]
                        E --> C
                        F[User Query] --> A
                        C --> G[Final Answer]
                        
                        style A fill:#667eea,color:#fff
                        style G fill:#43e97b,color:#fff
                </div>

                <div class="code-block" data-lang="Python">
<pre>from llama_index.llms.ibm import WatsonxLLM

# Create LLM
llm = WatsonxLLM(
    model_id="ibm/granite-3-2-8b-instruct",
    url="https://us-south.ml.cloud.ibm.com",
    project_id="your-project-id",
    temperature=0.0,
    max_new_tokens=500
)

# Create query engine
query_engine = index.as_query_engine(
    llm=llm,
    similarity_top_k=3,  # Retrieve top 3 most similar chunks
    streaming=False
)

# Query the index
response = query_engine.query("What is LlamaIndex?")
print(response.response)

# Access source nodes
for node in response.source_nodes:
    print(f"Score: {node.score:.3f}")
    print(f"Text: {node.text[:100]}...")
</pre>
                </div>
            </div>

            <div class="card">
                <h3>6. Prompt Templates</h3>
                
                <p>Prompt templates control how context and queries are formatted before being sent to the LLM.</p>

                <div class="code-block" data-lang="Python">
<pre>from llama_index.core import PromptTemplate

# Define a custom QA template
qa_template = PromptTemplate(
    """Context information is below.
    ---------------------
    {context_str}
    ---------------------
    
    Given the context information and not prior knowledge,
    answer the question: {query_str}
    
    If the answer is not in the context, say "I don't know."
    """
)

# Use custom template in query engine
query_engine = index.as_query_engine(
    llm=llm,
    text_qa_template=qa_template,
    similarity_top_k=5
)

response = query_engine.query("How does vector search work?")
</pre>
                </div>
            </div>
        </section>

        <!-- Architecture Section -->
        <section id="architecture">
            <h2>üèóÔ∏è LlamaIndex Architecture</h2>

            <div class="card">
                <h3>System Architecture Overview</h3>
                
                <div class="mermaid">
                    graph TB
                        subgraph "Data Layer"
                            A1[File System]
                            A2[Databases]
                            A3[APIs]
                            A4[Cloud Storage]
                        end
                        
                        subgraph "Ingestion Layer"
                            B1[SimpleDirectoryReader]
                            B2[DatabaseReader]
                            B3[Custom Loaders]
                            A1 --> B1
                            A2 --> B2
                            A3 --> B3
                            A4 --> B1
                        end
                        
                        subgraph "Processing Layer"
                            C1[Text Splitter]
                            C2[Node Parser]
                            C3[Metadata Extractor]
                            B1 --> C1
                            B2 --> C1
                            B3 --> C1
                            C1 --> C2
                            C2 --> C3
                        end
                        
                        subgraph "Embedding Layer"
                            D1[Embedding Model]
                            D2[Batch Processor]
                            C3 --> D1
                            D1 --> D2
                        end
                        
                        subgraph "Storage Layer"
                            E1[Vector Store]
                            E2[Document Store]
                            E3[Index Store]
                            D2 --> E1
                            C3 --> E2
                            E1 --> E3
                        end
                        
                        subgraph "Query Layer"
                            F1[Retriever]
                            F2[Reranker]
                            F3[Query Engine]
                            E3 --> F1
                            F1 --> F2
                            F2 --> F3
                        end
                        
                        subgraph "Generation Layer"
                            G1[Prompt Builder]
                            G2[LLM Interface]
                            G3[Response Synthesizer]
                            F3 --> G1
                            G1 --> G2
                            G2 --> G3
                        end
                        
                        H[User Query] --> F3
                        G3 --> I[Final Response]
                        
                        style B1 fill:#667eea,color:#fff
                        style D1 fill:#f093fb,color:#fff
                        style E1 fill:#4facfe,color:#fff
                        style G2 fill:#43e97b,color:#fff
                        style I fill:#f59e0b,color:#fff
                </div>
            </div>

            <div class="card">
                <h3>Component Interaction Flow</h3>
                
                <div class="mermaid">
                    sequenceDiagram
                        participant U as User
                        participant QE as Query Engine
                        participant R as Retriever
                        participant VS as Vector Store
                        participant LLM as Language Model
                        
                        U->>QE: Submit Query
                        QE->>R: Embed Query
                        R->>VS: Search Similar Vectors
                        VS-->>R: Return Top-K Nodes
                        R-->>QE: Retrieved Context
                        QE->>QE: Build Augmented Prompt
                        QE->>LLM: Send Prompt + Context
                        LLM-->>QE: Generated Response
                        QE-->>U: Final Answer
                </div>
            </div>

            <div class="card">
                <h3>Storage Architecture</h3>
                
                <div class="table-container">
                    <table>
                        <thead>
                            <tr>
                                <th>Store Type</th>
                                <th>Purpose</th>
                                <th>Implementation Options</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Vector Store</strong></td>
                                <td>Stores embeddings for semantic search</td>
                                <td>In-memory, ChromaDB, Faiss, Pinecone, Milvus, Weaviate</td>
                            </tr>
                            <tr>
                                <td><strong>Document Store</strong></td>
                                <td>Stores original document text and metadata</td>
                                <td>MongoDB, PostgreSQL, Redis, DynamoDB</td>
                            </tr>
                            <tr>
                                <td><strong>Index Store</strong></td>
                                <td>Stores index structure and node relationships</td>
                                <td>JSON files, SQL databases, NoSQL stores</td>
                            </tr>
                            <tr>
                                <td><strong>Graph Store</strong></td>
                                <td>Stores relationships between documents/nodes</td>
                                <td>Neo4j, NetworkX, NebulaGraph</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </section>

        <!-- Implementation Section -->
        <section id="implementation">
            <h2>üíª Implementation Guide: Building an AI Icebreaker Bot</h2>

            <div class="card">
                <h3>Project Overview</h3>
                <p>We'll build a complete RAG application that analyzes LinkedIn profiles and generates personalized conversation starters. This project demonstrates all key LlamaIndex concepts in a real-world scenario.</p>

                <div class="highlight-box">
                    <h4>üéØ Project Goals</h4>
                    <ul>
                        <li>Extract and process LinkedIn profile data</li>
                        <li>Create a searchable vector index of profile information</li>
                        <li>Generate interesting facts as icebreakers</li>
                        <li>Answer specific questions about the profile</li>
                        <li>Deploy with both CLI and web interfaces</li>
                    </ul>
                </div>
            </div>

            <div class="card">
                <h3>Step 1: Environment Setup</h3>
                
                <div class="code-block" data-lang="Bash">
<pre># Create project directory
mkdir icebreaker && cd icebreaker

# Create virtual environment
python3.11 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install ibm-watsonx-ai
pip install llama-index
pip install llama-index-core
pip install llama-index-llms-ibm
pip install llama-index-embeddings-ibm
pip install gradio requests
</pre>
                </div>
            </div>

            <div class="card">
                <h3>Step 2: Configuration Module (config.py)</h3>
                
                <div class="code-block" data-lang="Python">
<pre># config.py
"""Configuration settings for the Icebreaker Bot."""

# IBM watsonx Configuration
WATSONX_URL = "https://us-south.ml.cloud.ibm.com"
WATSONX_PROJECT_ID = "your-project-id"

# Model Configuration
LLM_MODEL_ID = "ibm/granite-3-2-8b-instruct"
EMBEDDING_MODEL_ID = "ibm/slate-125m-english-rtrvr"

# LLM Parameters
TEMPERATURE = 0.0
MAX_NEW_TOKENS = 500
MIN_NEW_TOKENS = 50
TOP_K = 50
TOP_P = 0.9

# RAG Configuration
CHUNK_SIZE = 512
CHUNK_OVERLAP = 50
SIMILARITY_TOP_K = 5

# Mock Data URL
MOCK_DATA_URL = "https://example.com/mock-profile.json"

# Prompt Templates
INITIAL_FACTS_TEMPLATE = """You are an AI assistant analyzing a professional's LinkedIn profile.

Context information about the person is below.
---------------------
{context_str}
---------------------

Based on the context, generate 3 interesting facts about this person's career or education.
Focus on unique accomplishments, interesting career transitions, or notable achievements.

Make the facts engaging and suitable as conversation starters.

Answer:"""

USER_QUESTION_TEMPLATE = """You are an AI assistant with knowledge about a professional's LinkedIn profile.

Context information is below.
---------------------
{context_str}
---------------------

Given this context, answer the following question: {query_str}

If the answer is not in the context, say "I don't know based on the available information."

Answer:"""
</pre>
                </div>
            </div>

            <div class="card">
                <h3>Step 3: Data Extraction Module (data_extraction.py)</h3>
                
                <div class="code-block" data-lang="Python">
<pre># data_extraction.py
"""Module for extracting LinkedIn profile data."""

import json
import logging
import requests
from typing import Dict, Any, Optional
import config

logger = logging.getLogger(__name__)


def extract_linkedin_profile(
    linkedin_profile_url: str,
    api_key: Optional[str] = None,
    mock: bool = False
) -> Dict[str, Any]:
    """
    Extract LinkedIn profile data using API or mock data.
    
    Args:
        linkedin_profile_url: The LinkedIn profile URL
        api_key: API key for profile extraction service
        mock: If True, loads mock data instead of making API call
    
    Returns:
        Dictionary containing profile data
    """
    try:
        if mock:
            logger.info("Loading mock profile data...")
            response = requests.get(config.MOCK_DATA_URL, timeout=30)
            response.raise_for_status()
            data = response.json()
        else:
            if not api_key:
                raise ValueError("API key required when mock=False")
            
            logger.info(f"Extracting profile: {linkedin_profile_url}")
            
            # API call would go here
            # For this example, we'll simulate with mock data
            data = {
                "full_name": "John Doe",
                "headline": "Senior Data Scientist at TechCorp",
                "summary": "Experienced data scientist with expertise in ML and AI",
                "experiences": [
                    {
                        "title": "Senior Data Scientist",
                        "company": "TechCorp",
                        "duration": "2020 - Present",
                        "description": "Leading ML initiatives and team development"
                    }
                ],
                "education": [
                    {
                        "school": "MIT",
                        "degree": "PhD in Computer Science",
                        "field": "Machine Learning",
                        "years": "2015-2019"
                    }
                ],
                "skills": ["Python", "Machine Learning", "TensorFlow", "Leadership"]
            }
        
        # Clean the data
        cleaned_data = {
            k: v for k, v in data.items()
            if v not in ([], "", None)
        }
        
        logger.info("Profile data extracted successfully")
        return cleaned_data
        
    except Exception as e:
        logger.error(f"Error extracting profile: {e}")
        return {}
</pre>
                </div>
            </div>

            <div class="card">
                <h3>Step 4: Data Processing Module (data_processing.py)</h3>
                
                <div class="code-block" data-lang="Python">
<pre># data_processing.py
"""Module for processing and indexing profile data."""

import json
import logging
from typing import Dict, Any, List, Optional
from llama_index.core import Document, VectorStoreIndex
from llama_index.core.node_parser import SentenceSplitter
import config

logger = logging.getLogger(__name__)


def split_profile_data(profile_data: Dict[str, Any]) -> List:
    """
    Split profile data into nodes for indexing.
    
    Args:
        profile_data: Dictionary containing LinkedIn profile data
    
    Returns:
        List of document nodes
    """
    try:
        # Convert profile dictionary to JSON string
        profile_json = json.dumps(profile_data, indent=2)
        
        # Create a Document object
        document = Document(text=profile_json)
        
        # Initialize sentence splitter
        splitter = SentenceSplitter(
            chunk_size=config.CHUNK_SIZE,
            chunk_overlap=config.CHUNK_OVERLAP
        )
        
        # Split into nodes
        nodes = splitter.get_nodes_from_documents([document])
        
        logger.info(f"Created {len(nodes)} nodes from profile data")
        return nodes
        
    except Exception as e:
        logger.error(f"Error splitting profile data: {e}")
        return []


def create_vector_database(
    nodes: List,
    embed_model
) -> Optional[VectorStoreIndex]:
    """
    Create vector database from nodes.
    
    Args:
        nodes: List of document nodes
        embed_model: Embedding model instance
    
    Returns:
        VectorStoreIndex or None if creation fails
    """
    try:
        # Create index with embeddings
        index = VectorStoreIndex(
            nodes=nodes,
            embed_model=embed_model,
            show_progress=True
        )
        
        logger.info("Vector database created successfully")
        return index
        
    except Exception as e:
        logger.error(f"Error creating vector database: {e}")
        return None


def verify_embeddings(index: VectorStoreIndex) -> bool:
    """
    Verify that all nodes have valid embeddings.
    
    Args:
        index: VectorStoreIndex to verify
    
    Returns:
        True if all embeddings are valid
    """
    try:
        vector_store = index._storage_context.vector_store
        node_ids = list(index.index_struct.nodes_dict.keys())
        
        missing_embeddings = False
        for node_id in node_ids:
            embedding = vector_store.get(node_id)
            if embedding is None:
                logger.warning(f"Node {node_id} has missing embedding")
                missing_embeddings = True
        
        if not missing_embeddings:
            logger.info("All node embeddings verified successfully")
            return True
        else:
            logger.warning("Some embeddings are missing")
            return False
            
    except Exception as e:
        logger.error(f"Error verifying embeddings: {e}")
        return False
</pre>
                </div>
            </div>

            <div class="card">
                <h3>Step 5: LLM Interface Module (llm_interface.py)</h3>
                
                <div class="code-block" data-lang="Python">
<pre># llm_interface.py
"""Module for interfacing with IBM watsonx models."""

import logging
from llama_index.llms.ibm import WatsonxLLM
from llama_index.embeddings.ibm import WatsonxEmbeddings
import config

logger = logging.getLogger(__name__)


def create_watsonx_embedding() -> WatsonxEmbeddings:
    """
    Create IBM watsonx embedding model.
    
    Returns:
        WatsonxEmbeddings instance
    """
    embedding_model = WatsonxEmbeddings(
        model_id=config.EMBEDDING_MODEL_ID,
        url=config.WATSONX_URL,
        project_id=config.WATSONX_PROJECT_ID,
        truncate_input_tokens=3
    )
    
    logger.info(f"Created embedding model: {config.EMBEDDING_MODEL_ID}")
    return embedding_model


def create_watsonx_llm(
    temperature: float = config.TEMPERATURE,
    max_new_tokens: int = config.MAX_NEW_TOKENS,
    decoding_method: str = "sample"
) -> WatsonxLLM:
    """
    Create IBM watsonx LLM.
    
    Args:
        temperature: Controls randomness (0.0 = deterministic)
        max_new_tokens: Maximum tokens to generate
        decoding_method: 'sample' or 'greedy'
    
    Returns:
        WatsonxLLM instance
    """
    additional_params = {
        "decoding_method": decoding_method,
        "min_new_tokens": config.MIN_NEW_TOKENS,
        "top_k": config.TOP_K,
        "top_p": config.TOP_P
    }
    
    llm = WatsonxLLM(
        model_id=config.LLM_MODEL_ID,
        url=config.WATSONX_URL,
        project_id=config.WATSONX_PROJECT_ID,
        temperature=temperature,
        max_new_tokens=max_new_tokens,
        additional_params=additional_params
    )
    
    logger.info(f"Created LLM: {config.LLM_MODEL_ID}")
    return llm


def change_llm_model(new_model_id: str) -> None:
    """
    Change the LLM model ID in configuration.
    
    Args:
        new_model_id: New model identifier
    """
    config.LLM_MODEL_ID = new_model_id
    logger.info(f"Changed LLM model to: {new_model_id}")
</pre>
                </div>
            </div>

            <div class="card">
                <h3>Step 6: Query Engine Module (query_engine.py)</h3>
                
                <div class="code-block" data-lang="Python">
<pre># query_engine.py
"""Module for querying the vector index and generating responses."""

import logging
from typing import Any
from llama_index.core import VectorStoreIndex, PromptTemplate
from llm_interface import create_watsonx_llm
import config

logger = logging.getLogger(__name__)


def generate_initial_facts(index: VectorStoreIndex) -> str:
    """
    Generate interesting facts about the profile.
    
    Args:
        index: VectorStoreIndex containing profile data
    
    Returns:
        String containing three interesting facts
    """
    try:
        # Create LLM with low temperature for factual output
        llm = create_watsonx_llm(
            temperature=0.0,
            max_new_tokens=500,
            decoding_method="sample"
        )
        
        # Create prompt template
        facts_prompt = PromptTemplate(
            template=config.INITIAL_FACTS_TEMPLATE
        )
        
        # Create query engine
        query_engine = index.as_query_engine(
            streaming=False,
            similarity_top_k=config.SIMILARITY_TOP_K,
            llm=llm,
            text_qa_template=facts_prompt
        )
        
        # Generate facts
        query = "Provide three interesting facts about this person's career or education."
        response = query_engine.query(query)
        
        logger.info("Generated initial facts successfully")
        return response.response
        
    except Exception as e:
        logger.error(f"Error generating facts: {e}")
        return "Failed to generate initial facts."


def answer_user_query(index: VectorStoreIndex, user_query: str) -> Any:
    """
    Answer a specific question about the profile.
    
    Args:
        index: VectorStoreIndex containing profile data
        user_query: User's question
    
    Returns:
        Response object with answer and source nodes
    """
    try:
        # Create LLM optimized for Q&A
        llm = create_watsonx_llm(
            temperature=0.0,
            max_new_tokens=250,
            decoding_method="greedy"
        )
        
        # Create prompt template
        question_prompt = PromptTemplate(
            template=config.USER_QUESTION_TEMPLATE
        )
        
        # Create retriever
        retriever = index.as_retriever(
            similarity_top_k=config.SIMILARITY_TOP_K
        )
        
        # Retrieve relevant nodes
        source_nodes = retriever.retrieve(user_query)
        
        # Build context string
        context_str = "\n\n".join([
            node.node.get_text() for node in source_nodes
        ])
        
        logger.info(f"Retrieved {len(source_nodes)} relevant nodes")
        
        # Create query engine
        query_engine = index.as_query_engine(
            streaming=False,
            similarity_top_k=config.SIMILARITY_TOP_K,
            llm=llm,
            text_qa_template=question_prompt
        )
        
        # Get answer
        answer = query_engine.query(user_query)
        
        logger.info("Query answered successfully")
        return answer
        
    except Exception as e:
        logger.error(f"Error answering query: {e}")
        return "Failed to get an answer."
</pre>
                </div>
            </div>

            <div class="card">
                <h3>Step 7: Main Application (main.py)</h3>
                
                <div class="code-block" data-lang="Python">
<pre># main.py
"""Main application for the Icebreaker Bot."""

import sys
import logging
import argparse
from data_extraction import extract_linkedin_profile
from data_processing import (
    split_profile_data,
    create_vector_database,
    verify_embeddings
)
from llm_interface import create_watsonx_embedding, change_llm_model
from query_engine import generate_initial_facts, answer_user_query
import config

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)


def process_linkedin(linkedin_url: str, api_key: str = None, mock: bool = False):
    """
    Process a LinkedIn profile through the RAG pipeline.
    
    Args:
        linkedin_url: LinkedIn profile URL
        api_key: API key for profile extraction
        mock: Whether to use mock data
    """
    try:
        logger.info("="*50)
        logger.info("ICEBREAKER BOT - RAG Pipeline Starting")
        logger.info("="*50)
        
        # Step 1: Extract profile data
        logger.info("\n[1/6] Extracting profile data...")
        profile_data = extract_linkedin_profile(linkedin_url, api_key, mock=mock)
        
        if not profile_data:
            logger.error("Failed to retrieve profile data")
            return
        
        # Step 2: Split data into nodes
        logger.info("\n[2/6] Splitting data into nodes...")
        nodes = split_profile_data(profile_data)
        
        if not nodes:
            logger.error("Failed to create nodes")
            return
        
        # Step 3: Create embedding model
        logger.info("\n[3/6] Initializing embedding model...")
        embed_model = create_watsonx_embedding()
        
        # Step 4: Create vector database
        logger.info("\n[4/6] Creating vector database...")
        index = create_vector_database(nodes, embed_model)
        
        if not index:
            logger.error("Failed to create vector database")
            return
        
        # Step 5: Verify embeddings
        logger.info("\n[5/6] Verifying embeddings...")
        if not verify_embeddings(index):
            logger.warning("Some embeddings may be invalid")
        
        # Step 6: Generate initial facts
        logger.info("\n[6/6] Generating conversation starters...")
        facts = generate_initial_facts(index)
        
        print("\n" + "="*50)
        print("‚ú® ICEBREAKER FACTS ‚ú®")
        print("="*50)
        print(facts)
        print("="*50)
        
        # Start interactive chat
        chatbot_interface(index)
        
    except Exception as e:
        logger.error(f"Error in process_linkedin: {e}")


def chatbot_interface(index):
    """
    Interactive chatbot interface for asking questions.
    
    Args:
        index: VectorStoreIndex containing profile data
    """
    print("\n\nüí¨ You can now ask questions about this person!")
    print("Type 'exit', 'quit', or 'bye' to end the conversation.\n")
    
    while True:
        try:
            user_query = input("You: ").strip()
            
            if user_query.lower() in ['exit', 'quit', 'bye']:
                print("Bot: Goodbye! Have great conversations! üëã")
                break
            
            if not user_query:
                continue
            
            print("Bot: Thinking...", end='\r')
            response = answer_user_query(index, user_query)
            print(" " * 50, end='\r')  # Clear "Thinking..."
            print(f"Bot: {response.response}\n")
            
        except KeyboardInterrupt:
            print("\n\nBot: Goodbye! üëã")
            break
        except Exception as e:
            logger.error(f"Error in chat: {e}")
            print("Bot: Sorry, I encountered an error. Please try again.\n")


def main():
    """Main entry point for the application."""
    parser = argparse.ArgumentParser(
        description='AI Icebreaker Bot - LinkedIn Profile Analyzer'
    )
    parser.add_argument(
        '--url',
        type=str,
        help='LinkedIn profile URL'
    )
    parser.add_argument(
        '--api-key',
        type=str,
        help='API key for profile extraction'
    )
    parser.add_argument(
        '--mock',
        action='store_true',
        help='Use mock data instead of live API'
    )
    parser.add_argument(
        '--model',
        type=str,
        help='LLM model to use (e.g., "ibm/granite-3-2-8b-instruct")'
    )
    
    args = parser.parse_args()
    
    # Change model if specified
    if args.model:
        change_llm_model(args.model)
    
    # Get LinkedIn URL
    linkedin_url = args.url or input("Enter LinkedIn profile URL (or press Enter for mock): ")
    use_mock = args.mock or not linkedin_url
    
    if use_mock and not linkedin_url:
        linkedin_url = "https://www.linkedin.com/in/example/"
    
    # Get API key if needed
    api_key = args.api_key if not use_mock else None
    
    # Process the profile
    process_linkedin(linkedin_url, api_key, mock=use_mock)


if __name__ == "__main__":
    main()
</pre>
                </div>
            </div>

            <div class="card">
                <h3>Step 8: Web Interface with Gradio (app.py)</h3>
                
                <div class="code-block" data-lang="Python">
<pre># app.py
"""Gradio web interface for the Icebreaker Bot."""

import sys
import logging
import uuid
import gradio as gr
from data_extraction import extract_linkedin_profile
from data_processing import (
    split_profile_data,
    create_vector_database,
    verify_embeddings
)
from llm_interface import (
    create_watsonx_embedding,
    change_llm_model
)
from query_engine import generate_initial_facts, answer_user_query
import config

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# Store active conversations
active_indices = {}


def process_profile(linkedin_url, api_key, use_mock, selected_model):
    """
    Process a LinkedIn profile and generate initial facts.
    
    Args:
        linkedin_url: LinkedIn profile URL
        api_key: API key for extraction service
        use_mock: Whether to use mock data
        selected_model: LLM model to use
    
    Returns:
        Tuple of (facts_text, session_id)
    """
    try:
        # Change model if needed
        if selected_model != config.LLM_MODEL_ID:
            change_llm_model(selected_model)
        
        # Use default URL for mock data
        if use_mock and not linkedin_url:
            linkedin_url = "https://www.linkedin.com/in/example/"
        
        # Extract profile data
        profile_data = extract_linkedin_profile(
            linkedin_url,
            api_key if not use_mock else None,
            mock=use_mock
        )
        
        if not profile_data:
            return "Failed to retrieve profile data. Please check your input.", None
        
        # Split into nodes
        nodes = split_profile_data(profile_data)
        if not nodes:
            return "Failed to process profile data.", None
        
        # Create embedding model
        embed_model = create_watsonx_embedding()
        
        # Create vector database
        index = create_vector_database(nodes, embed_model)
        if not index:
            return "Failed to create vector database.", None
        
        # Verify embeddings
        if not verify_embeddings(index):
            logger.warning("Some embeddings may be missing")
        
        # Generate initial facts
        facts = generate_initial_facts(index)
        
        # Create session ID
        session_id = str(uuid.uuid4())
        active_indices[session_id] = index
        
        result_text = f"‚úÖ Profile processed successfully!\n\n"
        result_text += "üéØ **Interesting Facts:**\n\n{facts}"
        
        return result_text, session_id
        
    except Exception as e:
        logger.error(f"Error processing profile: {e}")
        return f"Error: {str(e)}", None


def chat_with_profile(session_id, user_query, chat_history):
    """
    Chat with a processed LinkedIn profile.
    
    Args:
        session_id: Session identifier
        user_query: User's question
        chat_history: Previous conversation history
    
    Returns:
        Updated chat history
    """
    if not session_id:
        return chat_history + [[user_query, "‚ö†Ô∏è No profile loaded. Please process a profile first."]]
    
    if session_id not in active_indices:
        return chat_history + [[user_query, "‚ö†Ô∏è Session expired. Please process the profile again."]]
    
    if not user_query.strip():
        return chat_history
    
    try:
        # Get the index for this session
        index = active_indices[session_id]
        
        # Answer the query
        response = answer_user_query(index, user_query)
        
        # Update chat history
        return chat_history + [[user_query, response.response]]
        
    except Exception as e:
        logger.error(f"Error in chat: {e}")
        return chat_history + [[user_query, f"‚ùå Error: {str(e)}"]]


def create_gradio_interface():
    """Create and configure the Gradio web interface."""
    
    # Available models
    available_models = [
        "ibm/granite-3-2-8b-instruct",
        "meta-llama/llama-3-3-70b-instruct"
    ]
    
    with gr.Blocks(title="LinkedIn Icebreaker Bot", theme=gr.themes.Soft()) as demo:
        gr.Markdown(
            """
            # üöÄ LinkedIn Icebreaker Bot
            ### Generate personalized conversation starters from LinkedIn profiles
            
            This AI-powered tool uses RAG (Retrieval-Augmented Generation) to analyze
            LinkedIn profiles and create engaging icebreakers for networking events.
            """
        )
        
        with gr.Tab("üìä Process Profile"):
            with gr.Row():
                with gr.Column():
                    linkedin_url = gr.Textbox(
                        label="LinkedIn Profile URL",
                        placeholder="https://www.linkedin.com/in/username/",
                        lines=1
                    )
                    
                    api_key = gr.Textbox(
                        label="API Key (Leave empty for mock data)",
                        placeholder="Your API Key",
                        type="password",
                        value=""
                    )
                    
                    use_mock = gr.Checkbox(
                        label="Use Mock Data",
                        value=True,
                        info="Enable this to use pre-loaded sample data"
                    )
                    
                    model_dropdown = gr.Dropdown(
                        choices=available_models,
                        label="Select LLM Model",
                        value=config.LLM_MODEL_ID,
                        info="Choose the AI model for generating responses"
                    )
                    
                    process_btn = gr.Button(
                        "üîç Process Profile",
                        variant="primary",
                        size="lg"
                    )
                
                with gr.Column():
                    result_text = gr.Textbox(
                        label="Results",
                        lines=15,
                        placeholder="Initial facts will appear here..."
                    )
                    
                    session_id = gr.Textbox(
                        label="Session ID",
                        visible=False
                    )
            
            process_btn.click(
                fn=process_profile,
                inputs=[linkedin_url, api_key, use_mock, model_dropdown],
                outputs=[result_text, session_id]
            )
        
        with gr.Tab("üí¨ Chat"):
            gr.Markdown(
                """
                ### Ask questions about the processed profile
                
                You can ask specific questions like:
                - "What is their current role?"
                - "Where did they study?"
                - "What are their key skills?"
                """
            )
            
            chatbot = gr.Chatbot(
                height=500,
                bubble_full_width=False,
                avatar_images=(None, "ü§ñ")
            )
            
            with gr.Row():
                chat_input = gr.Textbox(
                    label="Your Question",
                    placeholder="Ask a question about the profile...",
                    scale=4
                )
                chat_btn = gr.Button("Send", variant="primary", scale=1)
            
            chat_btn.click(
                fn=chat_with_profile,
                inputs=[session_id, chat_input, chatbot],
                outputs=[chatbot]
            ).then(
                lambda: "",
                None,
                chat_input
            )
            
            chat_input.submit(
                fn=chat_with_profile,
                inputs=[session_id, chat_input, chatbot],
                outputs=[chatbot]
            ).then(
                lambda: "",
                None,
                chat_input
            )
        
        with gr.Tab("‚ÑπÔ∏è About"):
            gr.Markdown(
                """
                ## About This Application
                
                This Icebreaker Bot demonstrates a complete RAG (Retrieval-Augmented Generation) 
                application built with:
                
                - **LlamaIndex**: For data orchestration and RAG pipeline
                - **IBM watsonx**: For LLM and embedding models
                - **Gradio**: For the web interface
                
                ### How It Works
                
                1. **Load**: Extract LinkedIn profile data
                2. **Split**: Break data into chunks (nodes)
                3. **Embed**: Convert text to vector embeddings
                4. **Index**: Store embeddings in a searchable database
                5. **Query**: Retrieve relevant information and generate responses
                
                ### Features
                
                - ‚úÖ Automated data extraction and processing
                - ‚úÖ Vector-based semantic search
                - ‚úÖ Context-aware response generation
                - ‚úÖ Interactive Q&A interface
                - ‚úÖ Multiple LLM model support
                
                ---
                
                **Note**: This is a demonstration application. For production use, 
                implement proper authentication, rate limiting, and error handling.
                """
            )
    
    return demo


if __name__ == "__main__":
    demo = create_gradio_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=5000,
        share=True,
        show_error=True
    )
</pre>
                </div>
            </div>

            <div class="card">
                <h3>Running the Application</h3>
                
                <div class="info-box info">
                    <strong>Command Line Interface:</strong>
                    <div class="code-block" data-lang="Bash">
<pre># Using mock data
python main.py --mock

# With specific model
python main.py --mock --model "meta-llama/llama-3-3-70b-instruct"

# Interactive mode
python main.py
</pre>
                    </div>
                </div>

                <div class="info-box success">
                    <strong>Web Interface:</strong>
                    <div class="code-block" data-lang="Bash">
<pre># Start the Gradio web application
python app.py

# Access at http://localhost:5000
</pre>
                    </div>
                </div>
            </div>
        </section>

        <!-- Comparison Section -->
        <section id="comparison">
            <h2>‚öñÔ∏è LlamaIndex vs LangChain: Detailed Comparison</h2>

            <div class="card">
                <h3>Philosophy and Design</h3>
                
                <div class="mermaid">
                    graph LR
                        subgraph LangChain
                            A1[Component 1] --> A2[Component 2]
                            A2 --> A3[Component 3]
                            A3 --> A4[Output]
                        end
                        
                        subgraph LlamaIndex
                            B1[Integrated Pipeline] --> B4[Output]
                        end
                        
                        style A1 fill:#f093fb
                        style A2 fill:#f093fb
                        style A3 fill:#f093fb
                        style B1 fill:#43e97b
                </div>

                <div class="comparison-table">
                    <div class="comparison-item">
                        <h4>üîó LangChain</h4>
                        <p><strong>Philosophy:</strong> Modular, chain-based approach</p>
                        <p><strong>Strength:</strong> Maximum flexibility and control</p>
                        <p><strong>Best For:</strong> Complex workflows requiring customization</p>
                    </div>
                    
                    <div class="comparison-item">
                        <h4>ü¶ô LlamaIndex</h4>
                        <p><strong>Philosophy:</strong> Integrated, pipeline-based approach</p>
                        <p><strong>Strength:</strong> Simplicity and ease of development</p>
                        <p><strong>Best For:</strong> RAG applications and quick prototyping</p>
                    </div>
                </div>
            </div>

            <div class="card">
                <h3>Feature-by-Feature Comparison</h3>
                
                <div class="table-container">
                    <table>
                        <thead>
                            <tr>
                                <th>Feature</th>
                                <th>LangChain</th>
                                <th>LlamaIndex</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Document Loading</strong></td>
                                <td>Multiple specific loaders (TextLoader, CSVLoader, PDFLoader)</td>
                                <td>SimpleDirectoryReader handles many formats natively</td>
                            </tr>
                            <tr>
                                <td><strong>Text Splitting</strong></td>
                                <td>CharacterTextSplitter, RecursiveCharacterTextSplitter</td>
                                <td>SentenceSplitter (token-based), SemanticSplitter</td>
                            </tr>
                            <tr>
                                <td><strong>Embedding & Storage</strong></td>
                                <td>Separate steps: embed first, then store</td>
                                <td>Combined in VectorStoreIndex (single command)</td>
                            </tr>
                            <tr>
                                <td><strong>Vector Stores</strong></td>
                                <td>Direct integrations exposing unique features</td>
                                <td>Wrapped by VectorStoreIndex for backend agnosticism</td>
                            </tr>
                            <tr>
                                <td><strong>Metadata</strong></td>
                                <td>Manual setup, varies by backend</td>
                                <td>Automatic creation and storage</td>
                            </tr>
                            <tr>
                                <td><strong>Querying</strong></td>
                                <td>Separate retrieval and LLM steps</td>
                                <td>Query Engine combines all steps</td>
                            </tr>
                            <tr>
                                <td><strong>Prompt Templates</strong></td>
                                <td>Easy to customize, separate component</td>
                                <td>Combined with query engine, slightly harder to customize</td>
                            </tr>
                            <tr>
                                <td><strong>Learning Curve</strong></td>
                                <td>Steeper, more concepts to learn</td>
                                <td>Gentler, fewer abstractions</td>
                            </tr>
                            <tr>
                                <td><strong>Flexibility</strong></td>
                                <td>Very high, granular control</td>
                                <td>Moderate, balanced with simplicity</td>
                            </tr>
                            <tr>
                                <td><strong>Development Speed</strong></td>
                                <td>Slower, more boilerplate</td>
                                <td>Faster, less code required</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <div class="card">
                <h3>Code Comparison: Same Task, Different Frameworks</h3>
                
                <h4>LangChain Approach</h4>
                <div class="code-block" data-lang="Python">
<pre># LangChain: More steps, more control
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# Step 1: Load documents
loader = TextLoader('data.txt')
documents = loader.load()

# Step 2: Split documents
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
docs = text_splitter.split_documents(documents)

# Step 3: Create embeddings
embeddings = OpenAIEmbeddings()

# Step 4: Create vector store
vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=embeddings
)

# Step 5: Create retriever
retriever = vectorstore.as_retriever()

# Step 6: Create QA chain
llm = OpenAI(temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever
)

# Step 7: Query
response = qa_chain.run("What is this about?")
</pre>
                </div>

                <h4>LlamaIndex Approach</h4>
                <div class="code-block" data-lang="Python">
<pre># LlamaIndex: Fewer steps, more integrated
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.llms.openai import OpenAI

# Step 1: Load documents (handles multiple formats)
documents = SimpleDirectoryReader('data').load_data()

# Step 2: Create index (splitting + embedding + storage combined)
index = VectorStoreIndex.from_documents(documents)

# Step 3: Create query engine (retrieval + generation combined)
llm = OpenAI(temperature=0)
query_engine = index.as_query_engine(llm=llm)

# Step 4: Query
response = query_engine.query("What is this about?")
</pre>
                </div>
            </div>

            <div class="pros-cons">
                <div class="pros">
                    <h4>‚úÖ When to Choose LlamaIndex</h4>
                    <ul>
                        <li>Building RAG applications quickly</li>
                        <li>Need powerful defaults and less boilerplate</li>
                        <li>Want backend-agnostic code</li>
                        <li>Prioritize development speed</li>
                        <li>Working with document-heavy applications</li>
                        <li>Want automatic metadata handling</li>
                        <li>Building chatbots or Q&A systems</li>
                    </ul>
                </div>
                
                <div class="cons">
                    <h4>‚úÖ When to Choose LangChain</h4>
                    <ul>
                        <li>Need maximum flexibility and control</li>
                        <li>Building complex, multi-step workflows</li>
                        <li>Require specific vector store features</li>
                        <li>Want easy component customization</li>
                        <li>Building agents or autonomous systems</li>
                        <li>Need extensive third-party integrations</li>
                        <li>Working with diverse data sources</li>
                    </ul>
                </div>
            </div>

            <div class="card">
                <h3>Hybrid Approach</h3>
                
                <div class="info-box success">
                    <strong>Good News:</strong> You don't have to choose just one! LlamaIndex provides a <code>LangChainNodeParser</code> that wraps LangChain text splitters, allowing you to use LangChain's splitting strategies within LlamaIndex's pipeline.
                </div>

                <div class="code-block" data-lang="Python">
<pre>from llama_index.core.node_parser import LangchainNodeParser
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Use LangChain's splitter in LlamaIndex
langchain_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

node_parser = LangchainNodeParser(langchain_splitter)
nodes = node_parser.get_nodes_from_documents(documents)

# Continue with LlamaIndex workflow
index = VectorStoreIndex(nodes)
</pre>
                </div>
            </div>
        </section>

        <!-- Use Cases Section -->
        <section id="use-cases">
            <h2>üéØ Use Cases and Real-World Applications</h2>

            <div class="grid">
                <div class="card">
                    <h3>1. Enterprise Knowledge Management</h3>
                    <p><strong>Scenario:</strong> A large corporation wants to make internal documentation searchable and queryable.</p>
                    
                    <h4>Implementation:</h4>
                    <ul>
                        <li>Ingest company wikis, PDFs, and SharePoint documents</li>
                        <li>Create a unified vector index across all sources</li>
                        <li>Deploy a chatbot that answers employee questions</li>
                        <li>Integrate with Slack or Microsoft Teams</li>
                    </ul>
                    
                    <div class="info-box info">
                        <strong>Benefits:</strong>
                        <ul>
                            <li>Reduced time spent searching for information</li>
                            <li>Improved employee productivity</li>
                            <li>Consistent answers across the organization</li>
                            <li>Easy onboarding for new employees</li>
                        </ul>
                    </div>
                </div>

                <div class="card">
                    <h3>2. Customer Support Automation</h3>
                    <p><strong>Scenario:</strong> An e-commerce company wants to automate responses to common customer queries.</p>
                    
                    <h4>Implementation:</h4>
                    <ul>
                        <li>Index product manuals, FAQs, and support tickets</li>
                        <li>Create a RAG-powered chatbot for the website</li>
                        <li>Integrate with ticketing system for escalation</li>
                        <li>Continuously learn from resolved tickets</li>
                    </ul>
                    
                    <div class="code-block" data-lang="Python">
<pre># Customer support RAG implementation
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex

# Load support documentation
docs = SimpleDirectoryReader(
    input_files=["faq.pdf", "user_manual.pdf"]
).load_data()

# Create index
index = VectorStoreIndex.from_documents(docs)

# Create customer support query engine
support_engine = index.as_query_engine(
    similarity_top_k=5,
    response_mode="compact"
)

# Handle customer query
customer_question = "How do I return a product?"
response = support_engine.query(customer_question)
print(response)
</pre>
                    </div>
                </div>

                <div class="card">
                    <h3>3. Legal Document Analysis</h3>
                    <p><strong>Scenario:</strong> A law firm needs to quickly analyze and extract information from thousands of legal documents.</p>
                    
                    <h4>Implementation:</h4>
                    <ul>
                        <li>Ingest contracts, case law, and legal briefs</li>
                        <li>Use semantic search to find relevant precedents</li>
                        <li>Generate summaries of complex legal documents</li>
                        <li>Extract key clauses and obligations</li>
                    </ul>
                    
                    <div class="info-box warning">
                        <strong>Important:</strong> For legal applications, always include human review and verification. RAG systems should augment, not replace, legal expertise.
                    </div>
                </div>

                <div class="card">
                    <h3>4. Medical Research Assistant</h3>
                    <p><strong>Scenario:</strong> Researchers need to quickly find relevant information across thousands of medical papers.</p>
                    
                    <h4>Implementation:</h4>
                    <ul>
                        <li>Index PubMed articles and clinical trial data</li>
                        <li>Enable semantic search across medical literature</li>
                        <li>Generate literature reviews automatically</li>
                        <li>Identify research gaps and opportunities</li>
                    </ul>
                </div>

                <div class="card">
                    <h3>5. Code Documentation Navigator</h3>
                    <p><strong>Scenario:</strong> A software company wants to make their codebase more accessible to developers.</p>
                    
                    <h4>Implementation:</h4>
                    <div class="code-block" data-lang="Python">
<pre># Index code repository
from llama_index.core import SimpleDirectoryReader
from llama_index.core.node_parser import CodeSplitter

# Load code files
documents = SimpleDirectoryReader(
    input_dir="./src",
    recursive=True,
    required_exts=[".py", ".js", ".java"]
).load_data()

# Use code-aware splitter
code_splitter = CodeSplitter(
    language="python",
    chunk_lines=50,
    chunk_lines_overlap=10
)

nodes = code_splitter.get_nodes_from_documents(documents)

# Create index
index = VectorStoreIndex(nodes)

# Query codebase
query_engine = index.as_query_engine()
response = query_engine.query(
    "How is user authentication implemented?"
)
</pre>
                    </div>
                </div>

                <div class="card">
                    <h3>6. Financial Report Analysis</h3>
                    <p><strong>Scenario:</strong> Investment analysts need to quickly extract insights from financial reports.</p>
                    
                    <h4>Implementation:</h4>
                    <ul>
                        <li>Index annual reports, 10-K filings, earnings calls</li>
                        <li>Extract financial metrics and trends</li>
                        <li>Compare companies side-by-side</li>
                        <li>Generate investment summaries</li>
                    </ul>
                </div>
            </div>

            <div class="card">
                <h3>Industry-Specific Applications</h3>
                
                <div class="table-container">
                    <table>
                        <thead>
                            <tr>
                                <th>Industry</th>
                                <th>Application</th>
                                <th>Key Benefits</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Healthcare</strong></td>
                                <td>Clinical decision support, medical literature search</td>
                                <td>Faster diagnosis, evidence-based care</td>
                            </tr>
                            <tr>
                                <td><strong>Finance</strong></td>
                                <td>Risk assessment, regulatory compliance</td>
                                <td>Improved accuracy, reduced risk</td>
                            </tr>
                            <tr>
                                <td><strong>Education</strong></td>
                                <td>Personalized tutoring, curriculum planning</td>
                                <td>Better learning outcomes, scalability</td>
                            </tr>
                            <tr>
                                <td><strong>Retail</strong></td>
                                <td>Product recommendations, inventory optimization</td>
                                <td>Increased sales, reduced costs</td>
                            </tr>
                            <tr>
                                <td><strong>Manufacturing</strong></td>
                                <td>Equipment manuals, troubleshooting guides</td>
                                <td>Reduced downtime, faster repairs</td>
                            </tr>
                            <tr>
                                <td><strong>Government</strong></td>
                                <td>Policy analysis, citizen services</td>
                                <td>Better transparency, improved services</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
			  <!-- LlamaIndex Infographics -->
            <div class="infographic-section">
                <h3>LlamaIndex Architecture & Flow</h3>
                <img src="https://raw.githubusercontent.com/md-sahil-analyst22/Study_Material/c950c2c1d152c74a2f2b84a1f063e6c2af0637d6/images/llmaindex_infographics.png" alt="LlamaIndex Infographics 1">
            </div>

            <div class="infographic-section">
                <h3>LlamaIndex Integration & Components</h3>
                <img src="https://raw.githubusercontent.com/md-sahil-analyst22/Study_Material/c950c2c1d152c74a2f2b84a1f063e6c2af0637d6/images/llmaindex_infographics2.png" alt="LlamaIndex Infographics 2">
            </div>
			
        </section>

        <!-- Pros and Cons Section -->
        <section id="pros-cons">
            <h2>‚öñÔ∏è Advantages and Limitations</h2>

            <div class="card">
                <h3>LlamaIndex Strengths</h3>
                
                <div class="pros-cons">
                    <div class="pros">
                        <h4>‚úÖ Advantages</h4>
                        <ul>
                            <li>Rapid development with sensible defaults</li>
                            <li>Unified API across different vector stores</li>
                            <li>Automatic metadata handling and storage</li>
                            <li>Powerful SimpleDirectoryReader for multiple formats</li>
                            <li>Combined embedding and indexing in single commands</li>
                            <li>Query engines simplify retrieval + generation</li>
                            <li>Great documentation and community support</li>
                            <li>Native support for advanced retrieval patterns</li>
                            <li>Optimized for production use</li>
                            <li>Lower learning curve for beginners</li>
                        </ul>
                    </div>
                    
                    <div class="cons">
                        <h4>‚ùå Limitations</h4>
                        <ul>
                            <li>Less flexibility than more modular frameworks</li>
                            <li>Customizing combined steps can be challenging</li>
                            <li>Fewer third-party integrations than LangChain</li>
                            <li>Prompt template customization is less intuitive</li>
                            <li>Primarily focused on RAG use cases</li>
                            <li>Limited agent capabilities compared to alternatives</li>
                            <li>Some advanced features require workarounds</li>
                            <li>Smaller ecosystem than LangChain</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="card">
                <h3>RAG System Challenges and Solutions</h3>
                
                <div class="table-container">
                    <table>
                        <thead>
                            <tr>
                                <th>Challenge</th>
                                <th>Description</th>
                                <th>Solution</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Chunk Size</strong></td>
                                <td>Too small = loss of context; Too large = noise</td>
                                <td>Experiment with 256-1024 tokens; use overlap; try semantic splitting</td>
                            </tr>
                            <tr>
                                <td><strong>Retrieval Quality</strong></td>
                                <td>Retrieved chunks may not be the most relevant</td>
                                <td>Use reranking models, increase top_k, improve metadata</td>
                            </tr>
                            <tr>
                                <td><strong>Hallucinations</strong></td>
                                <td>LLM generates information not in the context</td>
                                <td>Use explicit prompts, lower temperature, add verification</td>
                            </tr>
                            <tr>
                                <td><strong>Latency</strong></td>
                                <td>Slow response times affect user experience</td>
                                <td>Optimize embedding models, use caching, implement streaming</td>
                            </tr>
                            <tr>
                                <td><strong>Cost</strong></td>
                                <td>API calls and compute can be expensive</td>
                                <td>Use smaller models, batch processing, local embeddings</td>
                            </tr>
                            <tr>
                                <td><strong>Data Freshness</strong></td>
                                <td>Index becomes outdated as source data changes</td>
                                <td>Implement incremental indexing, schedule regular updates</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <div class="card">
                <h3>Best Practices for Production RAG Systems</h3>
                
                <div class="grid">
                    <div class="info-box info">
                        <h4>üìä Monitoring</h4>
                        <ul>
                            <li>Track query latency and throughput</li>
                            <li>Monitor retrieval quality metrics</li>
                            <li>Log failed queries for analysis</li>
                            <li>Measure user satisfaction</li>
                        </ul>
                    </div>
                    
                    <div class="info-box warning">
                        <h4>üîí Security</h4>
                        <ul>
                            <li>Implement proper authentication</li>
                            <li>Sanitize user inputs</li>
                            <li>Use role-based access control</li>
                            <li>Encrypt sensitive data</li>
                        </ul>
                    </div>
                    
                    <div class="info-box success">
                        <h4>‚ö° Performance</h4>
                        <ul>
                            <li>Use vector database caching</li>
                            <li>Implement connection pooling</li>
                            <li>Optimize chunk sizes</li>
                            <li>Consider async operations</li>
                        </ul>
                    </div>
                    
                    <div class="info-box danger">
                        <h4>üêõ Error Handling</h4>
                        <ul>
                            <li>Graceful degradation on failures</li>
                            <li>Retry logic for API calls</li>
                            <li>Clear error messages for users</li>
                            <li>Comprehensive logging</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <!-- Alternatives Section -->
        <section id="alternatives">
            <h2>üîÑ Alternatives to LlamaIndex</h2>

            <div class="card">
                <h3>Framework Comparison</h3>
                
                <div class="table-container">
                    <table>
                        <thead>
                            <tr>
                                <th>Framework</th>
                                <th>Strengths</th>
                                <th>Best For</th>
                                <th>Language</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>LlamaIndex</strong></td>
                                <td>Simplicity, RAG-focused, great defaults</td>
                                <td>RAG applications, document Q&A</td>
                                <td>Python, TypeScript</td>
                            </tr>
                            <tr>
                                <td><strong>LangChain</strong></td>
                                <td>Flexibility, extensive integrations, agents</td>
                                <td>Complex workflows, autonomous agents</td>
                                <td>Python, JavaScript</td>
                            </tr>
                            <tr>
                                <td><strong>Haystack</strong></td>
                                <td>Production-ready, pipeline-based, NLP focus</td>
                                <td>Enterprise search, QA systems</td>
                                <td>Python</td>
                            </tr>
                            <tr>
                                <td><strong>Semantic Kernel</strong></td>
                                <td>Microsoft ecosystem, enterprise features</td>
                                <td>.NET applications, Azure integration</td>
                                <td>C#, Python, Java</td>
                            </tr>
                            <tr>
                                <td><strong>txtai</strong></td>
                                <td>Lightweight, fast, semantic search</td>
                                <td>Embedded systems, edge devices</td>
                                <td>Python</td>
                            </tr>
                            <tr>
                                <td><strong>Weaviate Client</strong></td>
                                <td>Native vector database integration</td>
                                <td>Weaviate-specific applications</td>
                                <td>Python, Go, Java, JS</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <div class="card">
                <h3>DIY Approach: Building from Scratch</h3>
                
                <p>While frameworks like LlamaIndex simplify development, you can also build RAG systems from scratch. Here's what you'd need:</p>

                <div class="code-block" data-lang="Python">
<pre># Minimal RAG implementation without frameworks
import numpy as np
from openai import OpenAI
from sklearn.metrics.pairwise import cosine_similarity

client = OpenAI(api_key="your-key")

# 1. Create embeddings
def embed_text(text):
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding

# 2. Store documents with embeddings
documents = [
    "LlamaIndex is a data framework for LLMs",
    "RAG improves LLM responses with external data",
    "Vector databases enable semantic search"
]

doc_embeddings = [embed_text(doc) for doc in documents]

# 3. Retrieve relevant documents
def retrieve(query, top_k=2):
    query_emb = embed_text(query)
    
    # Calculate similarities
    similarities = cosine_similarity(
        [query_emb],
        doc_embeddings
    )[0]
    
    # Get top-k indices
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    
    return [documents[i] for i in top_indices]

# 4. Generate response with context
def rag_query(question):
    context_docs = retrieve(question)
    context = "\n".join(context_docs)
    
    prompt = f"""Context:
{context}

Question: {question}

Answer based on the context above:"""
    
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    
    return response.choices[0].message.content

# Use it
answer = rag_query("What is RAG?")
print(answer)
</pre>
                </div>

                <div class="info-box warning">
                    <strong>‚ö†Ô∏è DIY Trade-offs:</strong>
                    <ul>
                        <li><strong>Pros:</strong> Full control, no dependencies, understanding of internals</li>
                        <li><strong>Cons:</strong> More code to maintain, missing optimizations, reinventing wheels</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- Future Section -->
        <section id="future">
            <h2>üöÄ Future of RAG and LlamaIndex</h2>

            <div class="card">
                <h3>Emerging Trends in RAG Technology</h3>
                
                <div class="timeline">
                    <div class="timeline-item">
                        <div class="timeline-content">
                            <h4>üß† Advanced Retrieval Techniques</h4>
                            <p><strong>Current:</strong> Simple vector similarity search</p>
                            <p><strong>Future:</strong> Hybrid search (vector + keyword), graph-based retrieval, multi-hop reasoning</p>
                            <span class="badge badge-primary">Active Development</span>
                        </div>
                    </div>
                    
                    <div class="timeline-item">
                        <div class="timeline-content">
                            <h4>üîÑ Agentic RAG</h4>
                            <p><strong>Concept:</strong> RAG systems that can decide when and what to retrieve</p>
                            <p><strong>Impact:</strong> More intelligent, context-aware retrieval decisions</p>
                            <span class="badge badge-secondary">Emerging</span>
                        </div>
                    </div>
                    
                    <div class="timeline-item">
                        <div class="timeline-content">
                            <h4>üìä Multi-Modal RAG</h4>
                            <p><strong>Beyond Text:</strong> Images, audio, video, and structured data</p>
                            <p><strong>Applications:</strong> Visual search, video Q&A, cross-modal retrieval</p>
                            <span class="badge badge-success">Growing</span>
                        </div>
                    </div>
                    
                    <div class="timeline-item">
                        <div class="timeline-content">
                            <h4>‚ö° Real-Time RAG</h4>
                            <p><strong>Challenge:</strong> Keeping indexes updated with streaming data</p>
                            <p><strong>Solution:</strong> Incremental indexing, real-time vector updates</p>
                            <span class="badge badge-warning">In Progress</span>
                        </div>
                    </div>
                </div>
            </div>

            <div class="card">
                <h3>LlamaIndex Roadmap</h3>
                
                <div class="mermaid">
                    graph TB
                        A[Current State] --> B[Enhanced Query Engines]
                        A --> C[Better Agent Support]
                        A --> D[Multi-Modal Index]
                        
                        B --> E[Advanced Retrieval]
                        C --> F[Autonomous Systems]
                        D --> G[Vision + Text RAG]
                        
                        E --> H[Production RAG 2.0]
                        F --> H
                        G --> H
                        
                        style A fill:#667eea,color:#fff
                        style H fill:#43e97b,color:#fff
                </div>

                <h4>Key Areas of Development</h4>
                <ul>
                    <li><strong>Performance Optimization:</strong> Faster indexing, reduced latency, better caching</li>
                    <li><strong>Advanced Retrievers:</strong> Self-querying, hypothetical document embedding, parent-child retrieval</li>
                    <li><strong>Better Observability:</strong> Built-in metrics, tracing, and debugging tools</li>
                    <li><strong>Enterprise Features:</strong> RBAC, audit logs, compliance tools</li>
                    <li><strong>Easier Deployment:</strong> Containerization, cloud-native support, serverless options</li>
                </ul>
            </div>

            <div class="card">
                <h3>Industry Predictions</h3>
                
                <div class="grid">
                    <div class="highlight-box">
                        <h4>üìà 2025-2026</h4>
                        <ul>
                            <li>RAG becomes standard in enterprise AI</li>
                            <li>Hybrid search (vector + keyword) mainstream</li>
                            <li>Multi-modal RAG applications emerge</li>
                            <li>Better fine-tuning for retrieval models</li>
                        </ul>
                    </div>
                    
                    <div class="highlight-box">
                        <h4>üîÆ 2027-2028</h4>
                        <ul>
                            <li>Agentic RAG systems become common</li>
                            <li>Real-time, streaming RAG pipelines</li>
                            <li>Cross-lingual RAG capabilities</li>
                            <li>Personalized retrieval and generation</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="card">
                <h3>Research Directions</h3>
                
                <div class="table-container">
                    <table>
                        <thead>
                            <tr>
                                <th>Research Area</th>
                                <th>Current Challenge</th>
                                <th>Potential Solution</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Hallucination Reduction</strong></td>
                                <td>LLMs still generate false information</td>
                                <td>Attribution mechanisms, fact-checking layers, confidence scores</td>
                            </tr>
                            <tr>
                                <td><strong>Long-Context RAG</strong></td>
                                <td>Difficulty with very long documents</td>
                                <td>Hierarchical indexing, recursive summarization, attention mechanisms</td>
                            </tr>
                            <tr>
                                <td><strong>Retrieval Quality</strong></td>
                                <td>Not always getting the right chunks</td>
                                <td>Better embedding models, learned retrievers, query rewriting</td>
                            </tr>
                            <tr>
                                <td><strong>Explainability</strong></td>
                                <td>Black-box nature of retrieval + generation</td>
                                <td>Source attribution, confidence metrics, reasoning traces</td>
                            </tr>
                            <tr>
                                <td><strong>Efficiency</strong></td>
                                <td>High computational costs</td>
                                <td>Sparse embeddings, quantization, distillation</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <div class="card">
                <h3>Getting Involved</h3>
                
                <div class="info-box success">
                    <h4>ü§ù Join the Community</h4>
                    <ul>
                        <li><strong>GitHub:</strong> Contribute to LlamaIndex development</li>
                        <li><strong>Discord:</strong> Join discussions with other developers</li>
                        <li><strong>Twitter/X:</strong> Follow @llama_index for updates</li>
                        <li><strong>Blog:</strong> Read case studies and tutorials</li>
                        <li><strong>Workshops:</strong> Attend online training sessions</li>
                    </ul>
                </div>

                <div class="code-block" data-lang="Bash">
<pre># Stay updated with LlamaIndex
pip install --upgrade llama-index

# Check out the latest features
python -c "import llama_index; print(llama_index.__version__)"

# Explore examples
git clone https://github.com/run-llama/llama_index
cd llama_index/docs/examples
</pre>
                </div>
            </div>
        </section>

        <!-- Additional Resources Section -->
        <section id="resources">
            <h2>üìö Additional Resources and Learning Path</h2>

            <div class="card">
                <h3>Learning Path for Beginners</h3>
                
                <div class="mermaid">
                    graph LR
                        A[Fundamentals] --> B[Basic RAG]
                        B --> C[Advanced Techniques]
                        C --> D[Production Deployment]
                        
                        A1[Python Basics<br/>LLM Concepts] --> A
                        B1[Document Loading<br/>Vector Embeddings<br/>Simple Queries] --> B
                        C1[Custom Retrievers<br/>Prompt Engineering<br/>Optimization] --> C
                        D1[Monitoring<br/>Scaling<br/>Security] --> D
                        
                        style A fill:#667eea,color:#fff
                        style B fill:#f093fb,color:#fff
                        style C fill:#4facfe,color:#fff
                        style D fill:#43e97b,color:#fff
                </div>
            </div>

            <div class="card">
                <h3>Recommended Books and Courses</h3>
                
                <div class="grid">
                    <div class="feature-card">
                        <div class="feature-icon">üìñ</div>
                        <h4>Official Documentation</h4>
                        <p>LlamaIndex official docs - comprehensive guides and API references</p>
                        <span class="badge badge-primary">Free</span>
                    </div>
                    
                    <div class="feature-card">
                        <div class="feature-icon">üéì</div>
                        <h4>Online Courses</h4>
                        <p>Deeplearning.AI: Building Applications with Vector Databases</p>
                        <span class="badge badge-success">Free</span>
                    </div>
                    
                    <div class="feature-card">
                        <div class="feature-icon">üíª</div>
                        <h4>Hands-On Tutorials</h4>
                        <p>LlamaIndex GitHub examples and cookbook</p>
                        <span class="badge badge-primary">Free</span>
                    </div>
                    
                    <div class="feature-card">
                        <div class="feature-icon">üé¨</div>
                        <h4>Video Content</h4>
                        <p>YouTube tutorials and conference talks on RAG</p>
                        <span class="badge badge-success">Free</span>
                    </div>
                </div>
            </div>

            <div class="card">
                <h3>Quick Reference Guide</h3>
                
                <div class="code-block" data-lang="Python">
<pre># LlamaIndex Quick Reference

# 1. Installation
pip install llama-index llama-index-llms-ibm llama-index-embeddings-ibm

# 2. Basic RAG Pipeline
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex

documents = SimpleDirectoryReader("data").load_data()
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()
response = query_engine.query("Your question here")

# 3. Custom Splitter
from llama_index.core.node_parser import SentenceSplitter

splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)
nodes = splitter.get_nodes_from_documents(documents)

# 4. Custom LLM
from llama_index.llms.ibm import WatsonxLLM

llm = WatsonxLLM(model_id="ibm/granite-3-2-8b-instruct", temperature=0.0)
query_engine = index.as_query_engine(llm=llm)

# 5. Custom Embedding
from llama_index.embeddings.ibm import WatsonxEmbeddings

embed_model = WatsonxEmbeddings(model_id="ibm/slate-125m-english-rtrvr")
index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)

# 6. Persistent Storage
index.storage_context.persist(persist_dir="./storage")

# 7. Load from Storage
from llama_index.core import StorageContext, load_index_from_storage

storage_context = StorageContext.from_defaults(persist_dir="./storage")
index = load_index_from_storage(storage_context)

# 8. Streaming Responses
query_engine = index.as_query_engine(streaming=True)
response = query_engine.query("Your question")
for text in response.response_gen:
    print(text, end="")

# 9. Chat Engine
chat_engine = index.as_chat_engine()
response = chat_engine.chat("Hello!")
response = chat_engine.chat("Follow-up question")

# 10. Retriever
retriever = index.as_retriever(similarity_top_k=5)
nodes = retriever.retrieve("query")
for node in nodes:
    print(node.text)
</pre>
                </div>
            </div>

            <div class="card">
                <h3>Common Pitfalls and How to Avoid Them</h3>
                
                <div class="table-container">
                    <table>
                        <thead>
                            <tr>
                                <th>Pitfall</th>
                                <th>Problem</th>
                                <th>Solution</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Not saving indexes</strong></td>
                                <td>Re-indexing on every run wastes time and money</td>
                                <td>Use <code>index.storage_context.persist()</code></td>
                            </tr>
                            <tr>
                                <td><strong>Ignoring chunk size</strong></td>
                                <td>Poor retrieval quality</td>
                                <td>Experiment with 256-1024 tokens, use overlap</td>
                            </tr>
                            <tr>
                                <td><strong>Not validating inputs</strong></td>
                                <td>Security vulnerabilities, crashes</td>
                                <td>Sanitize user inputs, add validation</td>
                            </tr>
                            <tr>
                                <td><strong>Over-relying on defaults</strong></td>
                                <td>Sub-optimal performance</td>
                                <td>Tune top_k, temperature, and other parameters</td>
                            </tr>
                            <tr>
                                <td><strong>Missing error handling</strong></td>
                                <td>Poor user experience on failures</td>
                                <td>Add try-catch blocks and fallback responses</td>
                            </tr>
                            <tr>
                                <td><strong>Not monitoring usage</strong></td>
                                <td>Unexpected costs, performance issues</td>
                                <td>Implement logging, metrics, and alerts</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </section>

        <!-- Conclusion Section -->
        <section id="conclusion">
            <h2>üéì Conclusion</h2>

            <div class="card">
                <div class="highlight-box">
                    <h3>Key Takeaways</h3>
                    <ul>
                        <li>‚úÖ <strong>LlamaIndex</strong> is a powerful framework optimized for building RAG applications with ease and efficiency</li>
                        <li>‚úÖ <strong>RAG</strong> solves the fundamental problem of making LLMs aware of your specific data</li>
                        <li>‚úÖ The five-stage pipeline (Loading ‚Üí Splitting ‚Üí Indexing ‚Üí Storing ‚Üí Querying) forms the backbone of all RAG systems</li>
                        <li>‚úÖ <strong>Vector embeddings</strong> enable semantic search, finding information based on meaning rather than keywords</li>
                        <li>‚úÖ <strong>Query engines</strong> streamline the retrieval and generation process into a single, elegant interface</li>
                        <li>‚úÖ LlamaIndex excels at simplicity while LangChain offers more flexibility‚Äîchoose based on your needs</li>
                        <li>‚úÖ Production RAG systems require attention to monitoring, security, performance, and error handling</li>
                        <li>‚úÖ The future of RAG includes multi-modal capabilities, agentic systems, and real-time processing</li>
                    </ul>
                </div>
            </div>

            <div class="card">
                <h3>Next Steps</h3>
                
                <div class="grid">
                    <div class="info-box info">
                        <h4>üöÄ Build Your First Project</h4>
                        <p>Start with a simple document Q&A system using your own data. Follow the Icebreaker Bot tutorial as a template.</p>
                    </div>
                    
                    <div class="info-box success">
                        <h4>üìñ Deep Dive</h4>
                        <p>Explore advanced topics like custom retrievers, prompt optimization, and production deployment strategies.</p>
                    </div>
                    
                    <div class="info-box warning">
                        <h4>ü§ù Join the Community</h4>
                        <p>Connect with other developers on Discord, GitHub, and forums. Share your projects and learn from others.</p>
                    </div>
                    
                    <div class="info-box danger">
                        <h4>üî¨ Experiment</h4>
                        <p>Try different embedding models, chunking strategies, and LLMs. Benchmark performance and iterate.</p>
                    </div>
                </div>
            </div>

            <div class="card">
                <div class="highlight-box">
                    <h3>üåü Final Thoughts</h3>
                    <p>Retrieval-Augmented Generation represents a paradigm shift in how we build AI applications. By combining the reasoning capabilities of large language models with the precision of information retrieval, RAG enables applications that are both intelligent and grounded in factual data.</p>
                    
                    <p>LlamaIndex has emerged as the framework of choice for developers who want to build RAG applications quickly without sacrificing quality. Its thoughtful defaults, powerful abstractions, and production-ready features make it an excellent choice for projects ranging from simple prototypes to enterprise-scale deployments.</p>
                    
                    <p>As you embark on your RAG development journey, remember that the best way to learn is by building. Start small, iterate quickly, and don't be afraid to experiment. The RAG ecosystem is rapidly evolving, and there's never been a better time to get involved.</p>
                    
                    <p style="text-align: center; font-size: 1.5rem; margin-top: 2rem;"><strong>Happy Building! üöÄ</strong></p>
                </div>
            </div>
        </section>
    </div>

    <footer>
        <p>&copy; 2025 LlamaIndex & RAG Study Guide | Created for Educational Purposes</p>
        <p>Last Updated: December 2025 | Based on LlamaIndex 0.10+ and IBM watsonx</p>
    </footer>

    <script>
        // Initialize Mermaid
        mermaid.initialize({ 
            startOnLoad: true,
            theme: 'default',
            themeVariables: {
                primaryColor: '#667eea',
                primaryTextColor: '#fff',
                primaryBorderColor: '#764ba2',
                lineColor: '#667eea',
                secondaryColor: '#43e97b',
                tertiaryColor: '#f093fb'
            }
        });

        // Smooth scrolling for navigation
        document.querySelectorAll('nav a').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                target.scrollIntoView({ behavior: 'smooth', block: 'start' });
            });
        });

        // Add fade-in animation on scroll
        const observerOptions = {
            threshold: 0.1,
            rootMargin: '0px 0px -100px 0px'
        };

        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.style.opacity = '1';
                    entry.target.style.transform = 'translateY(0)';
                }
            });
        }, observerOptions);

        document.querySelectorAll('.card, .feature-card').forEach(el => {
            el.style.opacity = '0';
            el.style.transform = 'translateY(20px)';
            el.style.transition = 'opacity 0.6s ease, transform 0.6s ease';
            observer.observe(el);
        });

        // Code block copy functionality
        document.querySelectorAll('.code-block').forEach(block => {
            const button = document.createElement('button');
            button.textContent = 'üìã Copy';
            button.style.cssText = 'position: absolute; top: 0.5rem; left: 0.5rem; padding: 0.25rem 0.75rem; background: #10b981; color: white; border: none; border-radius: 4px; cursor: pointer; font-size: 0.75rem;';
            
            button.addEventListener('click', () => {
                const code = block.querySelector('pre').textContent;
                navigator.clipboard.write
